2024-11-18:12:46:48 [INFO    ] [ft-sam.py:234] {'adaptive': True,
 'alpha': 0.0,
 'amp': True,
 'batch_size': 128,
 'checkpoint_load': None,
 'checkpoint_save': 'record/sig_0_1_RN18FR/defense/ft-sam/checkpoint/',
 'client_optimizer': 'sgd',
 'dataset': 'cifar10',
 'dataset_path': './data/cifar10',
 'device': 'cuda:0',
 'epochs': 20,
 'frequency_save': 100,
 'img_size': (32, 32, 3),
 'index': None,
 'input_channel': 3,
 'input_height': 32,
 'input_width': 32,
 'label_smoothing': 0.1,
 'log': 'record/sig_0_1_RN18FR/defense/ft-sam/log/',
 'lr': 0.01,
 'lr_scheduler': 'CosineAnnealingLR',
 'model': 'preactresnet18',
 'non_blocking': True,
 'num_classes': 10,
 'num_workers': 4,
 'pin_memory': True,
 'prefetch': False,
 'print_freq': 1,
 'random_seed': 0,
 'ratio': 0.05,
 'result_file': 'sig_0_1_RN18FR',
 'rho': 2.0,
 'rho_max': 2.0,
 'rho_min': 2.0,
 'save_path': 'record/sig_0_1_RN18FR/defense/ft-sam/',
 'sgd_momentum': 0.9,
 'terminal_info': ['./defense/ft-sam.py',
                   '--result_file',
                   'sig_0_1_RN18FR',
                   '--yaml_path',
                   './config/defense/ft-sam/cifar10.yaml',
                   '--dataset',
                   'cifar10',
                   '--epochs',
                   '20',
                   '--ratio',
                   '0.05',
                   '--device',
                   'cuda:0',
                   '--model',
                   'preactresnet18'],
 'wd': 0.0005,
 'yaml_path': './config/defense/ft-sam/cifar10.yaml'}
2024-11-18:12:46:50 [INFO    ] [bd_dataset_v2.py:133] save file format is .png
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][2/20]	                     loss 1.8813188076019287 (1.8813188076019287	                     Acc@1 0.828125 (0.828125
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][3/20]	                     loss 1.3475688695907593 (1.614443838596344	                     Acc@1 0.9296875 (0.87890625
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][4/20]	                     loss 1.470682978630066 (1.566523551940918	                     Acc@1 0.859375 (0.8723958333333334
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][5/20]	                     loss 1.2704336643218994 (1.4925010800361633	                     Acc@1 0.890625 (0.876953125
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][6/20]	                     loss 1.052034616470337 (1.404407787322998	                     Acc@1 0.921875 (0.8859375
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][7/20]	                     loss 1.1535499095916748 (1.362598141034444	                     Acc@1 0.921875 (0.8919270833333334
2024-11-18:12:46:52 [INFO    ] [ft-sam.py:312] Train: [1][8/20]	                     loss 0.876923680305481 (1.2932160752160209	                     Acc@1 0.9296875 (0.8973214285714286
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][9/20]	                     loss 0.9340847134590149 (1.2483246549963951	                     Acc@1 0.921875 (0.900390625
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][10/20]	                     loss 0.9146729707717896 (1.2112522456381056	                     Acc@1 0.8671875 (0.8967013888888888
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][11/20]	                     loss 0.6907780170440674 (1.1592048227787017	                     Acc@1 0.9296875 (0.9
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][12/20]	                     loss 0.8638001680374146 (1.1323498541658574	                     Acc@1 0.8671875 (0.8970170454545454
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][13/20]	                     loss 0.7027950882911682 (1.0965536236763	                     Acc@1 0.8671875 (0.89453125
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][14/20]	                     loss 0.584503173828125 (1.0571651275341327	                     Acc@1 0.90625 (0.8954326923076923
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][15/20]	                     loss 0.63924640417099 (1.0273137901510512	                     Acc@1 0.875 (0.8939732142857143
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][16/20]	                     loss 0.6448566913604736 (1.0018166502316792	                     Acc@1 0.8203125 (0.8890625
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][17/20]	                     loss 0.4563438296318054 (0.9677245989441872	                     Acc@1 0.9140625 (0.890625
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][18/20]	                     loss 0.5139173865318298 (0.9410300570375779	                     Acc@1 0.875 (0.8897058823529411
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][19/20]	                     loss 0.4466990828514099 (0.9135672251383463	                     Acc@1 0.890625 (0.8897569444444444
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][20/20]	                     loss 0.49367934465408325 (0.8914678630075956	                     Acc@1 0.8203125 (0.8861019736842105
2024-11-18:12:46:53 [INFO    ] [ft-sam.py:312] Train: [1][21/20]	                     loss 0.464430034160614 (0.8798524340629578	                     Acc@1 0.8676470588235294 (0.8856
2024-11-18:12:46:56 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.178933922673615,
 'clean_test_loss_avg_over_batch': 0.679933979164196,
 'test_acc': 0.7827,
 'test_asr': 0.0005555555555555556,
 'test_ra': 0.5606666666666666,
 'train_acc': 0.8856,
 'train_epoch_loss_avg_over_batch': 0.8798524340629578}
2024-11-18:12:46:56 [INFO    ] [ft-sam.py:312] Train: [2][2/20]	                     loss 0.4025057554244995 (0.4025057554244995	                     Acc@1 0.8984375 (0.8984375
2024-11-18:12:46:56 [INFO    ] [ft-sam.py:312] Train: [2][3/20]	                     loss 0.3134254217147827 (0.3579655885696411	                     Acc@1 0.921875 (0.91015625
2024-11-18:12:46:56 [INFO    ] [ft-sam.py:312] Train: [2][4/20]	                     loss 0.33716142177581787 (0.3510308663050334	                     Acc@1 0.9375 (0.9192708333333334
2024-11-18:12:46:56 [INFO    ] [ft-sam.py:312] Train: [2][5/20]	                     loss 0.38222432136535645 (0.35882923007011414	                     Acc@1 0.8828125 (0.91015625
2024-11-18:12:46:56 [INFO    ] [ft-sam.py:312] Train: [2][6/20]	                     loss 0.33459174633026123 (0.35398173332214355	                     Acc@1 0.890625 (0.90625
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][7/20]	                     loss 0.3286825716495514 (0.34976520637671155	                     Acc@1 0.890625 (0.9036458333333334
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][8/20]	                     loss 0.3414764404296875 (0.3485810969557081	                     Acc@1 0.8828125 (0.9006696428571429
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][9/20]	                     loss 0.3511751890182495 (0.34890535846352577	                     Acc@1 0.890625 (0.8994140625
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][10/20]	                     loss 0.31874656677246094 (0.345554381608963	                     Acc@1 0.9296875 (0.9027777777777778
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][11/20]	                     loss 0.3008992075920105 (0.34108886420726775	                     Acc@1 0.9375 (0.90625
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][12/20]	                     loss 0.2919057309627533 (0.33661767027594824	                     Acc@1 0.9296875 (0.9083806818181818
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][13/20]	                     loss 0.2914333641529083 (0.3328523114323616	                     Acc@1 0.9375 (0.9108072916666666
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][14/20]	                     loss 0.3145661950111389 (0.33144568709226757	                     Acc@1 0.8984375 (0.9098557692307693
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][15/20]	                     loss 0.2716299891471863 (0.32717313723904745	                     Acc@1 0.953125 (0.9129464285714286
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][16/20]	                     loss 0.28431200981140137 (0.32431572874387105	                     Acc@1 0.921875 (0.9135416666666667
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][17/20]	                     loss 0.3029671907424927 (0.3229814451187849	                     Acc@1 0.890625 (0.912109375
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][18/20]	                     loss 0.300766259431839 (0.321674669490141	                     Acc@1 0.921875 (0.9126838235294118
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][19/20]	                     loss 0.2959444522857666 (0.3202452129787869	                     Acc@1 0.9296875 (0.9136284722222222
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][20/20]	                     loss 0.2437785267829895 (0.31622065054742915	                     Acc@1 0.9609375 (0.9161184210526315
2024-11-18:12:46:57 [INFO    ] [ft-sam.py:312] Train: [2][21/20]	                     loss 0.31842249631881714 (0.3162805407524109	                     Acc@1 0.8823529411764706 (0.9152
2024-11-18:12:47:00 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.159369475404981,
 'clean_test_loss_avg_over_batch': 0.48132632050333146,
 'test_acc': 0.8592,
 'test_asr': 0.00044444444444444447,
 'test_ra': 0.555,
 'train_acc': 0.9152,
 'train_epoch_loss_avg_over_batch': 0.3162805407524109}
2024-11-18:12:47:00 [INFO    ] [ft-sam.py:312] Train: [3][2/20]	                     loss 0.23056170344352722 (0.23056170344352722	                     Acc@1 0.9609375 (0.9609375
2024-11-18:12:47:00 [INFO    ] [ft-sam.py:312] Train: [3][3/20]	                     loss 0.29255935549736023 (0.2615605294704437	                     Acc@1 0.9296875 (0.9453125
2024-11-18:12:47:00 [INFO    ] [ft-sam.py:312] Train: [3][4/20]	                     loss 0.2365315556526184 (0.25321753819783527	                     Acc@1 0.953125 (0.9479166666666666
2024-11-18:12:47:00 [INFO    ] [ft-sam.py:312] Train: [3][5/20]	                     loss 0.21956832706928253 (0.2448052354156971	                     Acc@1 0.9609375 (0.951171875
2024-11-18:12:47:00 [INFO    ] [ft-sam.py:312] Train: [3][6/20]	                     loss 0.23530271649360657 (0.242904731631279	                     Acc@1 0.9609375 (0.953125
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][7/20]	                     loss 0.21684756875038147 (0.2385618711511294	                     Acc@1 0.9296875 (0.94921875
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][8/20]	                     loss 0.2093416154384613 (0.23438754890646255	                     Acc@1 0.984375 (0.9542410714285714
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][9/20]	                     loss 0.2219875305891037 (0.23283754661679268	                     Acc@1 0.9375 (0.9521484375
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][10/20]	                     loss 0.20894455909729004 (0.23018277022573683	                     Acc@1 0.96875 (0.9539930555555556
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][11/20]	                     loss 0.23776875436306 (0.23094136863946915	                     Acc@1 0.953125 (0.95390625
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][12/20]	                     loss 0.2539333403110504 (0.23303154788234018	                     Acc@1 0.953125 (0.9538352272727273
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][13/20]	                     loss 0.2114904224872589 (0.23123645409941673	                     Acc@1 0.953125 (0.9537760416666666
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][14/20]	                     loss 0.21987032890319824 (0.2303621367766307	                     Acc@1 0.9609375 (0.9543269230769231
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][15/20]	                     loss 0.2387077808380127 (0.23095825420958654	                     Acc@1 0.953125 (0.9542410714285714
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][16/20]	                     loss 0.21066045761108398 (0.2296050677696864	                     Acc@1 0.9609375 (0.9546875
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][17/20]	                     loss 0.2177668809890747 (0.22886518109589815	                     Acc@1 0.953125 (0.95458984375
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][18/20]	                     loss 0.23626643419265747 (0.22930054892511928	                     Acc@1 0.9375 (0.9535845588235294
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][19/20]	                     loss 0.24324125051498413 (0.23007503234677845	                     Acc@1 0.9453125 (0.953125
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][20/20]	                     loss 0.2360544353723526 (0.2303897377691771	                     Acc@1 0.9375 (0.9523026315789473
2024-11-18:12:47:01 [INFO    ] [ft-sam.py:312] Train: [3][21/20]	                     loss 0.20126143097877502 (0.22959744782447816	                     Acc@1 0.9852941176470589 (0.9532
2024-11-18:12:47:04 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.950342896958472,
 'clean_test_loss_avg_over_batch': 0.4173516416096989,
 'test_acc': 0.8837,
 'test_asr': 0.0015555555555555555,
 'test_ra': 0.5634444444444444,
 'train_acc': 0.9532,
 'train_epoch_loss_avg_over_batch': 0.22959744782447816}
2024-11-18:12:47:04 [INFO    ] [ft-sam.py:312] Train: [4][2/20]	                     loss 0.18390227854251862 (0.18390227854251862	                     Acc@1 0.953125 (0.953125
2024-11-18:12:47:04 [INFO    ] [ft-sam.py:312] Train: [4][3/20]	                     loss 0.19643354415893555 (0.19016791135072708	                     Acc@1 0.9609375 (0.95703125
2024-11-18:12:47:04 [INFO    ] [ft-sam.py:312] Train: [4][4/20]	                     loss 0.1749843806028366 (0.18510673443476358	                     Acc@1 0.984375 (0.9661458333333334
2024-11-18:12:47:04 [INFO    ] [ft-sam.py:312] Train: [4][5/20]	                     loss 0.18711981177330017 (0.18561000376939774	                     Acc@1 0.953125 (0.962890625
2024-11-18:12:47:04 [INFO    ] [ft-sam.py:312] Train: [4][6/20]	                     loss 0.22664874792099 (0.19381775259971618	                     Acc@1 0.9609375 (0.9625
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][7/20]	                     loss 0.17492219805717468 (0.1906684935092926	                     Acc@1 0.984375 (0.9661458333333334
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][8/20]	                     loss 0.26460951566696167 (0.2012314966746739	                     Acc@1 0.953125 (0.9642857142857143
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][9/20]	                     loss 0.21431922912597656 (0.20286746323108673	                     Acc@1 0.953125 (0.962890625
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][10/20]	                     loss 0.1848943531513214 (0.2008704510000017	                     Acc@1 0.96875 (0.9635416666666666
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][11/20]	                     loss 0.15213608741760254 (0.19599701464176178	                     Acc@1 0.984375 (0.965625
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][12/20]	                     loss 0.20349714159965515 (0.19667884436520663	                     Acc@1 0.984375 (0.9673295454545454
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][13/20]	                     loss 0.1860870122909546 (0.1957961916923523	                     Acc@1 0.9765625 (0.9680989583333334
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][14/20]	                     loss 0.15590758621692657 (0.19272783742501184	                     Acc@1 0.9921875 (0.9699519230769231
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][15/20]	                     loss 0.19973167777061462 (0.19322811173541204	                     Acc@1 0.96875 (0.9698660714285714
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][16/20]	                     loss 0.21674948930740356 (0.19479620357354482	                     Acc@1 0.9375 (0.9677083333333333
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][17/20]	                     loss 0.18195568025112152 (0.19399367086589336	                     Acc@1 0.9765625 (0.96826171875
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][18/20]	                     loss 0.19078409671783447 (0.19380487238659577	                     Acc@1 0.9453125 (0.9669117647058824
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][19/20]	                     loss 0.21336030960083008 (0.19489128556516436	                     Acc@1 0.96875 (0.9670138888888888
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][20/20]	                     loss 0.1671724170446396 (0.19343239774829463	                     Acc@1 0.9765625 (0.967516447368421
2024-11-18:12:47:05 [INFO    ] [ft-sam.py:312] Train: [4][21/20]	                     loss 0.1872280091047287 (0.19326363837718963	                     Acc@1 0.9705882352941176 (0.9676
2024-11-18:12:47:08 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.74655522091288,
 'clean_test_loss_avg_over_batch': 0.40438321157346796,
 'test_acc': 0.8898,
 'test_asr': 0.001888888888888889,
 'test_ra': 0.5894444444444444,
 'train_acc': 0.9676,
 'train_epoch_loss_avg_over_batch': 0.19326363837718963}
2024-11-18:12:47:08 [INFO    ] [ft-sam.py:312] Train: [5][2/20]	                     loss 0.2089611291885376 (0.2089611291885376	                     Acc@1 0.9453125 (0.9453125
2024-11-18:12:47:08 [INFO    ] [ft-sam.py:312] Train: [5][3/20]	                     loss 0.18868258595466614 (0.19882185757160187	                     Acc@1 0.9609375 (0.953125
2024-11-18:12:47:08 [INFO    ] [ft-sam.py:312] Train: [5][4/20]	                     loss 0.17904631793498993 (0.19223001102606455	                     Acc@1 0.96875 (0.9583333333333334
2024-11-18:12:47:08 [INFO    ] [ft-sam.py:312] Train: [5][5/20]	                     loss 0.19911763072013855 (0.19395191594958305	                     Acc@1 0.96875 (0.9609375
2024-11-18:12:47:08 [INFO    ] [ft-sam.py:312] Train: [5][6/20]	                     loss 0.18235933780670166 (0.19163340032100679	                     Acc@1 0.9609375 (0.9609375
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][7/20]	                     loss 0.19489902257919312 (0.19217767069737116	                     Acc@1 0.9609375 (0.9609375
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][8/20]	                     loss 0.1747797727584839 (0.18969225670610154	                     Acc@1 0.953125 (0.9598214285714286
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][9/20]	                     loss 0.18403175473213196 (0.18898469395935535	                     Acc@1 0.9609375 (0.9599609375
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][10/20]	                     loss 0.16258741915225983 (0.18605166342523363	                     Acc@1 0.984375 (0.9626736111111112
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][11/20]	                     loss 0.18024742603302002 (0.18547123968601226	                     Acc@1 0.96875 (0.96328125
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][12/20]	                     loss 0.14317454397678375 (0.18162608553062787	                     Acc@1 0.9921875 (0.9659090909090909
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][13/20]	                     loss 0.18138104677200317 (0.1816056656340758	                     Acc@1 0.9765625 (0.966796875
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][14/20]	                     loss 0.15286210179328918 (0.1793946222617076	                     Acc@1 1.0 (0.9693509615384616
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][15/20]	                     loss 0.17559242248535156 (0.17912303656339645	                     Acc@1 0.9609375 (0.96875
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][16/20]	                     loss 0.17942924797534943 (0.17914345065752665	                     Acc@1 0.9765625 (0.9692708333333333
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][17/20]	                     loss 0.2048492431640625 (0.18075006268918514	                     Acc@1 0.953125 (0.96826171875
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][18/20]	                     loss 0.2002200484275818 (0.18189535596791437	                     Acc@1 0.9453125 (0.9669117647058824
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][19/20]	                     loss 0.19179199635982513 (0.18244516932302052	                     Acc@1 0.953125 (0.9661458333333334
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][20/20]	                     loss 0.18711236119270325 (0.18269081100037224	                     Acc@1 0.953125 (0.9654605263157895
2024-11-18:12:47:09 [INFO    ] [ft-sam.py:312] Train: [5][21/20]	                     loss 0.17122939229011536 (0.18237906041145324	                     Acc@1 0.9705882352941176 (0.9656
2024-11-18:12:47:12 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.758953181790634,
 'clean_test_loss_avg_over_batch': 0.39570155928406536,
 'test_acc': 0.8927,
 'test_asr': 0.0017777777777777779,
 'test_ra': 0.5905555555555555,
 'train_acc': 0.9656,
 'train_epoch_loss_avg_over_batch': 0.18237906041145324}
2024-11-18:12:47:12 [INFO    ] [ft-sam.py:312] Train: [6][2/20]	                     loss 0.18009597063064575 (0.18009597063064575	                     Acc@1 0.9765625 (0.9765625
2024-11-18:12:47:12 [INFO    ] [ft-sam.py:312] Train: [6][3/20]	                     loss 0.16592666506767273 (0.17301131784915924	                     Acc@1 0.984375 (0.98046875
2024-11-18:12:47:12 [INFO    ] [ft-sam.py:312] Train: [6][4/20]	                     loss 0.21270889043807983 (0.1862438420454661	                     Acc@1 0.9609375 (0.9739583333333334
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][5/20]	                     loss 0.1801612824201584 (0.18472320213913918	                     Acc@1 0.9765625 (0.974609375
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][6/20]	                     loss 0.15243002772331238 (0.1782645672559738	                     Acc@1 0.984375 (0.9765625
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][7/20]	                     loss 0.17745286226272583 (0.1781292830904325	                     Acc@1 0.984375 (0.9778645833333334
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][8/20]	                     loss 0.23779353499412537 (0.18665274764810288	                     Acc@1 0.921875 (0.9698660714285714
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][9/20]	                     loss 0.1842990219593048 (0.18635853193700314	                     Acc@1 0.9765625 (0.970703125
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][10/20]	                     loss 0.18712081015110016 (0.18644322951634726	                     Acc@1 0.9609375 (0.9696180555555556
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][11/20]	                     loss 0.18573951721191406 (0.18637285828590394	                     Acc@1 0.953125 (0.96796875
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][12/20]	                     loss 0.16977527737617493 (0.18486398729411038	                     Acc@1 0.9921875 (0.9701704545454546
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][13/20]	                     loss 0.1494237333536148 (0.18191063279906908	                     Acc@1 0.9765625 (0.970703125
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][14/20]	                     loss 0.17495356500148773 (0.18137547373771667	                     Acc@1 0.9765625 (0.9711538461538461
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][15/20]	                     loss 0.17392262816429138 (0.18084312762532914	                     Acc@1 0.9921875 (0.97265625
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][16/20]	                     loss 0.19442078471183777 (0.18174830476442974	                     Acc@1 0.9609375 (0.971875
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][17/20]	                     loss 0.21687328815460205 (0.1839436162263155	                     Acc@1 0.9609375 (0.97119140625
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][18/20]	                     loss 0.17005719244480133 (0.18312676776857936	                     Acc@1 0.96875 (0.9710477941176471
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][19/20]	                     loss 0.1522560566663742 (0.1814117282629013	                     Acc@1 0.984375 (0.9717881944444444
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][20/20]	                     loss 0.1671096384525299 (0.1806589866939344	                     Acc@1 0.9921875 (0.9728618421052632
2024-11-18:12:47:13 [INFO    ] [ft-sam.py:312] Train: [6][21/20]	                     loss 0.19067882001399994 (0.18093152616024016	                     Acc@1 0.9558823529411765 (0.9724
2024-11-18:12:47:16 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.711420683793619,
 'clean_test_loss_avg_over_batch': 0.39116829086708116,
 'test_acc': 0.8962,
 'test_asr': 0.0024444444444444444,
 'test_ra': 0.6166666666666667,
 'train_acc': 0.9724,
 'train_epoch_loss_avg_over_batch': 0.18093152616024016}
2024-11-18:12:47:16 [INFO    ] [ft-sam.py:312] Train: [7][2/20]	                     loss 0.1563803255558014 (0.1563803255558014	                     Acc@1 0.984375 (0.984375
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][3/20]	                     loss 0.17801707983016968 (0.16719870269298553	                     Acc@1 0.96875 (0.9765625
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][4/20]	                     loss 0.20223276317119598 (0.178876722852389	                     Acc@1 0.9609375 (0.9713541666666666
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][5/20]	                     loss 0.20146577060222626 (0.18452398478984833	                     Acc@1 0.9609375 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][6/20]	                     loss 0.20867356657981873 (0.1893539011478424	                     Acc@1 0.953125 (0.965625
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][7/20]	                     loss 0.1758885681629181 (0.18710967898368835	                     Acc@1 0.9609375 (0.96484375
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][8/20]	                     loss 0.16869564354419708 (0.18447910249233246	                     Acc@1 0.9609375 (0.9642857142857143
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][9/20]	                     loss 0.1990301012992859 (0.18629797734320164	                     Acc@1 0.96875 (0.96484375
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][10/20]	                     loss 0.1620386242866516 (0.18360249367025164	                     Acc@1 0.9921875 (0.9678819444444444
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][11/20]	                     loss 0.17265670001506805 (0.1825079143047333	                     Acc@1 0.9765625 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][12/20]	                     loss 0.18804433941841125 (0.183011225678704	                     Acc@1 0.96875 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][13/20]	                     loss 0.16857455670833588 (0.18180816993117332	                     Acc@1 0.984375 (0.9700520833333334
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][14/20]	                     loss 0.17120814323425293 (0.18099278326217944	                     Acc@1 0.9921875 (0.9717548076923077
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][15/20]	                     loss 0.18076740205287933 (0.1809766846043723	                     Acc@1 0.96875 (0.9715401785714286
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][16/20]	                     loss 0.21698404848575592 (0.18337717552979788	                     Acc@1 0.9296875 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][17/20]	                     loss 0.1764974296092987 (0.18294719140976667	                     Acc@1 0.96875 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][18/20]	                     loss 0.15758444368839264 (0.18145526507321527	                     Acc@1 0.9765625 (0.9692095588235294
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][19/20]	                     loss 0.16529355943202972 (0.18055739253759384	                     Acc@1 0.9609375 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][20/20]	                     loss 0.16802473366260529 (0.17989777891259445	                     Acc@1 0.96875 (0.96875
2024-11-18:12:47:17 [INFO    ] [ft-sam.py:312] Train: [7][21/20]	                     loss 0.19313620030879974 (0.18025786397457122	                     Acc@1 0.9705882352941176 (0.9688
2024-11-18:12:47:20 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.861150963205687,
 'clean_test_loss_avg_over_batch': 0.39550957159151007,
 'test_acc': 0.8939,
 'test_asr': 0.0014444444444444444,
 'test_ra': 0.5848888888888889,
 'train_acc': 0.9688,
 'train_epoch_loss_avg_over_batch': 0.18025786397457122}
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][2/20]	                     loss 0.1910473108291626 (0.1910473108291626	                     Acc@1 0.9609375 (0.9609375
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][3/20]	                     loss 0.1842590868473053 (0.18765319883823395	                     Acc@1 0.96875 (0.96484375
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][4/20]	                     loss 0.1858309954404831 (0.18704579770565033	                     Acc@1 0.9609375 (0.9635416666666666
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][5/20]	                     loss 0.1683535873889923 (0.18237274512648582	                     Acc@1 0.9765625 (0.966796875
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][6/20]	                     loss 0.16139289736747742 (0.17817677557468414	                     Acc@1 0.9765625 (0.96875
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][7/20]	                     loss 0.18560534715652466 (0.17941487083832422	                     Acc@1 0.9609375 (0.9674479166666666
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][8/20]	                     loss 0.16090181469917297 (0.17677014853273118	                     Acc@1 0.9609375 (0.9665178571428571
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][9/20]	                     loss 0.18251261115074158 (0.1774879563599825	                     Acc@1 0.9765625 (0.9677734375
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][10/20]	                     loss 0.15718865394592285 (0.17523247831397587	                     Acc@1 0.984375 (0.9696180555555556
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][11/20]	                     loss 0.1964803785085678 (0.17735726833343507	                     Acc@1 0.9609375 (0.96875
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][12/20]	                     loss 0.16546128690242767 (0.17627581547607074	                     Acc@1 0.9765625 (0.9694602272727273
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][13/20]	                     loss 0.16033600270748138 (0.17494749774535498	                     Acc@1 0.9765625 (0.9700520833333334
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][14/20]	                     loss 0.16103455424308777 (0.17387727132210365	                     Acc@1 0.9921875 (0.9717548076923077
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][15/20]	                     loss 0.16956321895122528 (0.17356912472418376	                     Acc@1 0.9609375 (0.9709821428571429
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][16/20]	                     loss 0.15783923864364624 (0.17252046565214793	                     Acc@1 0.984375 (0.971875
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][17/20]	                     loss 0.16907823085784912 (0.17230532597750425	                     Acc@1 0.9609375 (0.97119140625
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][18/20]	                     loss 0.13576656579971313 (0.17015598714351654	                     Acc@1 1.0 (0.9728860294117647
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][19/20]	                     loss 0.13836225867271423 (0.16838966889513862	                     Acc@1 0.984375 (0.9735243055555556
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][20/20]	                     loss 0.18583717942237854 (0.1693079589228881	                     Acc@1 0.9453125 (0.9720394736842105
2024-11-18:12:47:21 [INFO    ] [ft-sam.py:312] Train: [8][21/20]	                     loss 0.1845787614583969 (0.16972332475185395	                     Acc@1 0.9852941176470589 (0.9724
2024-11-18:12:47:24 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.992435965739506,
 'clean_test_loss_avg_over_batch': 0.3940648391276975,
 'test_acc': 0.893,
 'test_asr': 0.0005555555555555556,
 'test_ra': 0.6114444444444445,
 'train_acc': 0.9724,
 'train_epoch_loss_avg_over_batch': 0.16972332475185395}
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][2/20]	                     loss 0.14381414651870728 (0.14381414651870728	                     Acc@1 0.984375 (0.984375
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][3/20]	                     loss 0.135857492685318 (0.13983581960201263	                     Acc@1 0.9921875 (0.98828125
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][4/20]	                     loss 0.1608273833990097 (0.146833007534345	                     Acc@1 0.96875 (0.9817708333333334
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][5/20]	                     loss 0.1934407651424408 (0.15848494693636894	                     Acc@1 0.96875 (0.978515625
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][6/20]	                     loss 0.13703730702400208 (0.15419541895389557	                     Acc@1 0.984375 (0.9796875
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][7/20]	                     loss 0.1751691997051239 (0.15769104907910028	                     Acc@1 0.9609375 (0.9765625
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][8/20]	                     loss 0.14760136604309082 (0.1562496657882418	                     Acc@1 0.984375 (0.9776785714285714
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][9/20]	                     loss 0.19024807214736938 (0.16049946658313274	                     Acc@1 0.9609375 (0.9755859375
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][10/20]	                     loss 0.16895972192287445 (0.16143949495421517	                     Acc@1 0.9765625 (0.9756944444444444
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][11/20]	                     loss 0.18603894114494324 (0.16389943957328795	                     Acc@1 0.953125 (0.9734375
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][12/20]	                     loss 0.16723127663135529 (0.1642023338512941	                     Acc@1 0.984375 (0.9744318181818182
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][13/20]	                     loss 0.14305379986763 (0.16243995601932207	                     Acc@1 1.0 (0.9765625
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][14/20]	                     loss 0.1736045777797699 (0.16329877307781807	                     Acc@1 0.96875 (0.9759615384615384
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][15/20]	                     loss 0.16099813580513 (0.16313444184405462	                     Acc@1 0.953125 (0.9743303571428571
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][16/20]	                     loss 0.14035974442958832 (0.16161612868309022	                     Acc@1 0.9921875 (0.9755208333333333
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][17/20]	                     loss 0.1653270423412323 (0.1618480607867241	                     Acc@1 0.9765625 (0.9755859375
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][18/20]	                     loss 0.17519740760326385 (0.16263331648181467	                     Acc@1 0.96875 (0.9751838235294118
2024-11-18:12:47:25 [INFO    ] [ft-sam.py:312] Train: [9][19/20]	                     loss 0.1467878371477127 (0.16175301207436454	                     Acc@1 0.984375 (0.9756944444444444
2024-11-18:12:47:26 [INFO    ] [ft-sam.py:312] Train: [9][20/20]	                     loss 0.1353829801082611 (0.16036511565509595	                     Acc@1 0.9921875 (0.9765625
2024-11-18:12:47:26 [INFO    ] [ft-sam.py:312] Train: [9][21/20]	                     loss 0.11120090633630753 (0.1590278491616249	                     Acc@1 1.0 (0.9772
2024-11-18:12:47:28 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.767789236256774,
 'clean_test_loss_avg_over_batch': 0.39130198238771174,
 'test_acc': 0.8982,
 'test_asr': 0.0006666666666666666,
 'test_ra': 0.636,
 'train_acc': 0.9772,
 'train_epoch_loss_avg_over_batch': 0.1590278491616249}
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][2/20]	                     loss 0.18320190906524658 (0.18320190906524658	                     Acc@1 0.953125 (0.953125
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][3/20]	                     loss 0.1578931361436844 (0.17054752260446548	                     Acc@1 0.96875 (0.9609375
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][4/20]	                     loss 0.11268466711044312 (0.15125990410645804	                     Acc@1 1.0 (0.9739583333333334
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][5/20]	                     loss 0.15353961288928986 (0.15182983130216599	                     Acc@1 0.96875 (0.97265625
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][6/20]	                     loss 0.1710224747657776 (0.15566835999488832	                     Acc@1 0.9609375 (0.9703125
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][7/20]	                     loss 0.13394658267498016 (0.15204806377490362	                     Acc@1 0.984375 (0.97265625
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][8/20]	                     loss 0.12366632372140884 (0.14799352948154723	                     Acc@1 0.9921875 (0.9754464285714286
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][9/20]	                     loss 0.13664653897285461 (0.14657515566796064	                     Acc@1 1.0 (0.978515625
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][10/20]	                     loss 0.15005527436733246 (0.1469618355234464	                     Acc@1 0.9765625 (0.9782986111111112
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][11/20]	                     loss 0.15557700395584106 (0.14782335236668587	                     Acc@1 0.96875 (0.97734375
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][12/20]	                     loss 0.17239558696746826 (0.1500571918758479	                     Acc@1 0.9765625 (0.9772727272727273
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][13/20]	                     loss 0.12390720844268799 (0.14787802658975124	                     Acc@1 0.9921875 (0.978515625
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][14/20]	                     loss 0.15811915695667267 (0.1486658058487452	                     Acc@1 0.984375 (0.9789663461538461
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][15/20]	                     loss 0.1325358748435974 (0.14751366791980608	                     Acc@1 0.984375 (0.9793526785714286
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][16/20]	                     loss 0.1630076915025711 (0.14854660282532375	                     Acc@1 0.96875 (0.9786458333333333
2024-11-18:12:47:29 [INFO    ] [ft-sam.py:312] Train: [10][17/20]	                     loss 0.1326982080936432 (0.1475560781545937	                     Acc@1 0.984375 (0.97900390625
2024-11-18:12:47:30 [INFO    ] [ft-sam.py:312] Train: [10][18/20]	                     loss 0.14302054047584534 (0.14728928182054968	                     Acc@1 0.984375 (0.9793198529411765
2024-11-18:12:47:30 [INFO    ] [ft-sam.py:312] Train: [10][19/20]	                     loss 0.12637066841125488 (0.1461271366311444	                     Acc@1 0.9765625 (0.9791666666666666
2024-11-18:12:47:30 [INFO    ] [ft-sam.py:312] Train: [10][20/20]	                     loss 0.14881092309951782 (0.14626838855053248	                     Acc@1 0.984375 (0.9794407894736842
2024-11-18:12:47:30 [INFO    ] [ft-sam.py:312] Train: [10][21/20]	                     loss 0.2069198042154312 (0.14791810705661773	                     Acc@1 0.9411764705882353 (0.9784
2024-11-18:12:47:33 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.344395261415293,
 'clean_test_loss_avg_over_batch': 0.43057341175743297,
 'test_acc': 0.8846,
 'test_asr': 0.00011111111111111112,
 'test_ra': 0.5367777777777778,
 'train_acc': 0.9784,
 'train_epoch_loss_avg_over_batch': 0.14791810705661773}
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][2/20]	                     loss 0.15781059861183167 (0.15781059861183167	                     Acc@1 0.9609375 (0.9609375
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][3/20]	                     loss 0.12416988611221313 (0.1409902423620224	                     Acc@1 0.9765625 (0.96875
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][4/20]	                     loss 0.1167982965707779 (0.13292626043160757	                     Acc@1 1.0 (0.9791666666666666
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][5/20]	                     loss 0.12060175836086273 (0.12984513491392136	                     Acc@1 0.9921875 (0.982421875
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][6/20]	                     loss 0.14538657665252686 (0.13295342326164244	                     Acc@1 0.9609375 (0.978125
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][7/20]	                     loss 0.13183681666851044 (0.1327673221627871	                     Acc@1 0.9765625 (0.9778645833333334
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][8/20]	                     loss 0.1255972981452942 (0.13174303301743098	                     Acc@1 0.984375 (0.9787946428571429
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][9/20]	                     loss 0.13349458575248718 (0.131961977109313	                     Acc@1 0.9921875 (0.98046875
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][10/20]	                     loss 0.11833304166793823 (0.13044765094916025	                     Acc@1 0.984375 (0.9809027777777778
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][11/20]	                     loss 0.14502854645252228 (0.13190574049949647	                     Acc@1 0.96875 (0.9796875
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][12/20]	                     loss 0.15157896280288696 (0.13369421525435013	                     Acc@1 0.9765625 (0.9794034090909091
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][13/20]	                     loss 0.10444613546133041 (0.1312568752715985	                     Acc@1 0.9921875 (0.98046875
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][14/20]	                     loss 0.1300952285528183 (0.13116751783169234	                     Acc@1 0.9921875 (0.9813701923076923
2024-11-18:12:47:33 [INFO    ] [ft-sam.py:312] Train: [11][15/20]	                     loss 0.14883852005004883 (0.13242973227586066	                     Acc@1 0.9765625 (0.9810267857142857
2024-11-18:12:47:34 [INFO    ] [ft-sam.py:312] Train: [11][16/20]	                     loss 0.11829173564910889 (0.13148719916741053	                     Acc@1 0.9921875 (0.9817708333333334
2024-11-18:12:47:34 [INFO    ] [ft-sam.py:312] Train: [11][17/20]	                     loss 0.12424854189157486 (0.1310347830876708	                     Acc@1 0.984375 (0.98193359375
2024-11-18:12:47:34 [INFO    ] [ft-sam.py:312] Train: [11][18/20]	                     loss 0.13299641013145447 (0.13115017291377573	                     Acc@1 0.9765625 (0.9816176470588235
2024-11-18:12:47:34 [INFO    ] [ft-sam.py:312] Train: [11][19/20]	                     loss 0.12597712874412537 (0.13086278157101738	                     Acc@1 0.984375 (0.9817708333333334
2024-11-18:12:47:34 [INFO    ] [ft-sam.py:312] Train: [11][20/20]	                     loss 0.12253762036561966 (0.13042461519178591	                     Acc@1 0.9921875 (0.9823190789473685
2024-11-18:12:47:34 [INFO    ] [ft-sam.py:312] Train: [11][21/20]	                     loss 0.08989467471837997 (0.12932220081090928	                     Acc@1 1.0 (0.9828
2024-11-18:12:47:37 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.618898700660383,
 'clean_test_loss_avg_over_batch': 0.39598792791366577,
 'test_acc': 0.8927,
 'test_asr': 0.00011111111111111112,
 'test_ra': 0.5695555555555556,
 'train_acc': 0.9828,
 'train_epoch_loss_avg_over_batch': 0.12932220081090928}
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][2/20]	                     loss 0.14103475213050842 (0.14103475213050842	                     Acc@1 0.984375 (0.984375
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][3/20]	                     loss 0.12445780634880066 (0.13274627923965454	                     Acc@1 0.9765625 (0.98046875
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][4/20]	                     loss 0.17078539729118347 (0.14542598525683084	                     Acc@1 0.9375 (0.9661458333333334
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][5/20]	                     loss 0.11544093489646912 (0.13792972266674042	                     Acc@1 0.9921875 (0.97265625
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][6/20]	                     loss 0.081692174077034 (0.12668221294879914	                     Acc@1 1.0 (0.978125
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][7/20]	                     loss 0.10818015038967133 (0.12359853585561116	                     Acc@1 0.984375 (0.9791666666666666
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][8/20]	                     loss 0.10213068127632141 (0.1205316994871412	                     Acc@1 0.9921875 (0.9810267857142857
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][9/20]	                     loss 0.09737666696310043 (0.1176373204216361	                     Acc@1 1.0 (0.9833984375
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][10/20]	                     loss 0.1056915894150734 (0.11631001697646247	                     Acc@1 1.0 (0.9852430555555556
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][11/20]	                     loss 0.13143911957740784 (0.117822927236557	                     Acc@1 0.9765625 (0.984375
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][12/20]	                     loss 0.11848243325948715 (0.11788288232955066	                     Acc@1 1.0 (0.9857954545454546
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][13/20]	                     loss 0.130048930644989 (0.11889671968917052	                     Acc@1 0.9921875 (0.986328125
2024-11-18:12:47:37 [INFO    ] [ft-sam.py:312] Train: [12][14/20]	                     loss 0.09452584385871887 (0.11702203693298194	                     Acc@1 1.0 (0.9873798076923077
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][15/20]	                     loss 0.14511080086231232 (0.11902837721364838	                     Acc@1 0.984375 (0.9871651785714286
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][16/20]	                     loss 0.12769955396652222 (0.11960645566383997	                     Acc@1 0.984375 (0.9869791666666666
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][17/20]	                     loss 0.10140922665596008 (0.11846912885084748	                     Acc@1 1.0 (0.98779296875
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][18/20]	                     loss 0.12366172671318054 (0.1187745757839259	                     Acc@1 0.984375 (0.9875919117647058
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][19/20]	                     loss 0.11634344607591629 (0.1186395130223698	                     Acc@1 0.9921875 (0.9878472222222222
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][20/20]	                     loss 0.08631296455860138 (0.11693811573480305	                     Acc@1 1.0 (0.9884868421052632
2024-11-18:12:47:38 [INFO    ] [ft-sam.py:312] Train: [12][21/20]	                     loss 0.1252991110086441 (0.11716553480625153	                     Acc@1 1.0 (0.9888
2024-11-18:12:47:41 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.083231415547115,
 'clean_test_loss_avg_over_batch': 0.37893080635915827,
 'test_acc': 0.9021,
 'test_asr': 0.001,
 'test_ra': 0.5232222222222223,
 'train_acc': 0.9888,
 'train_epoch_loss_avg_over_batch': 0.11716553480625153}
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][2/20]	                     loss 0.1180075854063034 (0.1180075854063034	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][3/20]	                     loss 0.12109486758708954 (0.11955122649669647	                     Acc@1 0.984375 (0.98828125
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][4/20]	                     loss 0.09454594552516937 (0.11121613283952077	                     Acc@1 0.9921875 (0.9895833333333334
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][5/20]	                     loss 0.08976765722036362 (0.10585401393473148	                     Acc@1 1.0 (0.9921875
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][6/20]	                     loss 0.08890321105718613 (0.10246385335922241	                     Acc@1 1.0 (0.99375
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][7/20]	                     loss 0.11037162691354752 (0.1037818156182766	                     Acc@1 0.9921875 (0.9934895833333334
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][8/20]	                     loss 0.10722296684980392 (0.10427340865135193	                     Acc@1 0.9921875 (0.9933035714285714
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][9/20]	                     loss 0.1284421980381012 (0.10729450732469559	                     Acc@1 0.984375 (0.9921875
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][10/20]	                     loss 0.11487236618995667 (0.10813649164305793	                     Acc@1 0.984375 (0.9913194444444444
2024-11-18:12:47:41 [INFO    ] [ft-sam.py:312] Train: [13][11/20]	                     loss 0.1068943440914154 (0.10801227688789368	                     Acc@1 0.9921875 (0.99140625
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][12/20]	                     loss 0.12218959629535675 (0.10930112410675395	                     Acc@1 0.984375 (0.9907670454545454
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][13/20]	                     loss 0.11534899473190308 (0.10980511332551639	                     Acc@1 0.9765625 (0.9895833333333334
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][14/20]	                     loss 0.10146035999059677 (0.10916320922283027	                     Acc@1 0.9921875 (0.9897836538461539
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][15/20]	                     loss 0.10616251826286316 (0.10894887415426117	                     Acc@1 0.984375 (0.9893973214285714
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][16/20]	                     loss 0.11016003787517548 (0.10902961840232213	                     Acc@1 0.9765625 (0.9885416666666667
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][17/20]	                     loss 0.08839002251625061 (0.10773964365944266	                     Acc@1 1.0 (0.9892578125
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][18/20]	                     loss 0.10590910911560059 (0.10763196515686371	                     Acc@1 0.984375 (0.9889705882352942
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][19/20]	                     loss 0.08768397569656372 (0.10652374352018039	                     Acc@1 1.0 (0.9895833333333334
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][20/20]	                     loss 0.10313054174184799 (0.10634515395289973	                     Acc@1 1.0 (0.9901315789473685
2024-11-18:12:47:42 [INFO    ] [ft-sam.py:312] Train: [13][21/20]	                     loss 0.07073912769556046 (0.1053766700387001	                     Acc@1 1.0 (0.9904
2024-11-18:12:47:45 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.165203128062504,
 'clean_test_loss_avg_over_batch': 0.37725541523740264,
 'test_acc': 0.9032,
 'test_asr': 0.0003333333333333333,
 'test_ra': 0.5666666666666667,
 'train_acc': 0.9904,
 'train_epoch_loss_avg_over_batch': 0.1053766700387001}
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][2/20]	                     loss 0.11263249814510345 (0.11263249814510345	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][3/20]	                     loss 0.07493481785058975 (0.0937836579978466	                     Acc@1 1.0 (0.99609375
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][4/20]	                     loss 0.08238273859024048 (0.08998335152864456	                     Acc@1 1.0 (0.9973958333333334
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][5/20]	                     loss 0.09204590320587158 (0.09049898944795132	                     Acc@1 1.0 (0.998046875
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][6/20]	                     loss 0.09997960180044174 (0.0923951119184494	                     Acc@1 1.0 (0.9984375
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][7/20]	                     loss 0.10055829584598541 (0.09375564257303874	                     Acc@1 0.984375 (0.99609375
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][8/20]	                     loss 0.11085399985313416 (0.09619826504162379	                     Acc@1 0.96875 (0.9921875
2024-11-18:12:47:45 [INFO    ] [ft-sam.py:312] Train: [14][9/20]	                     loss 0.08673310279846191 (0.09501511976122856	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][10/20]	                     loss 0.08983699977397919 (0.09443977309597863	                     Acc@1 1.0 (0.9930555555555556
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][11/20]	                     loss 0.0972684696316719 (0.09472264274954796	                     Acc@1 0.9765625 (0.99140625
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][12/20]	                     loss 0.10364654660224915 (0.09553390673615715	                     Acc@1 0.9765625 (0.9900568181818182
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][13/20]	                     loss 0.09306499361991882 (0.09532816397647063	                     Acc@1 0.9921875 (0.990234375
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][14/20]	                     loss 0.1194789707660675 (0.09718591834490116	                     Acc@1 0.984375 (0.9897836538461539
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][15/20]	                     loss 0.10049404203891754 (0.09742221289447375	                     Acc@1 0.9921875 (0.9899553571428571
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][16/20]	                     loss 0.08467723429203033 (0.09657254765431086	                     Acc@1 0.9921875 (0.9901041666666667
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][17/20]	                     loss 0.11621895432472229 (0.09780044807121158	                     Acc@1 0.9765625 (0.9892578125
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][18/20]	                     loss 0.08825074881315231 (0.09723870105603162	                     Acc@1 0.9921875 (0.9894301470588235
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][19/20]	                     loss 0.09004546701908112 (0.0968390769428677	                     Acc@1 0.9921875 (0.9895833333333334
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][20/20]	                     loss 0.12242218106985092 (0.09818555610744577	                     Acc@1 0.96875 (0.9884868421052632
2024-11-18:12:47:46 [INFO    ] [ft-sam.py:312] Train: [14][21/20]	                     loss 0.09842238575220108 (0.09819199787378312	                     Acc@1 0.9852941176470589 (0.9884
2024-11-18:12:47:49 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.833567585743649,
 'clean_test_loss_avg_over_batch': 0.3707145961779582,
 'test_acc': 0.9037,
 'test_asr': 0.0012222222222222222,
 'test_ra': 0.5947777777777777,
 'train_acc': 0.9884,
 'train_epoch_loss_avg_over_batch': 0.09819199787378312}
2024-11-18:12:47:49 [INFO    ] [ft-sam.py:312] Train: [15][2/20]	                     loss 0.08650712668895721 (0.08650712668895721	                     Acc@1 1.0 (1.0
2024-11-18:12:47:49 [INFO    ] [ft-sam.py:312] Train: [15][3/20]	                     loss 0.15656998753547668 (0.12153855711221695	                     Acc@1 0.9375 (0.96875
2024-11-18:12:47:49 [INFO    ] [ft-sam.py:312] Train: [15][4/20]	                     loss 0.08507689088582993 (0.10938466837008794	                     Acc@1 1.0 (0.9791666666666666
2024-11-18:12:47:49 [INFO    ] [ft-sam.py:312] Train: [15][5/20]	                     loss 0.08294881880283356 (0.10277570597827435	                     Acc@1 1.0 (0.984375
2024-11-18:12:47:49 [INFO    ] [ft-sam.py:312] Train: [15][6/20]	                     loss 0.1243899017572403 (0.10709854513406754	                     Acc@1 0.9609375 (0.9796875
2024-11-18:12:47:49 [INFO    ] [ft-sam.py:312] Train: [15][7/20]	                     loss 0.0884324461221695 (0.10398752863208453	                     Acc@1 1.0 (0.9830729166666666
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][8/20]	                     loss 0.12096657603979111 (0.10641310683318547	                     Acc@1 0.9765625 (0.9821428571428571
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][9/20]	                     loss 0.09803630411624908 (0.10536600649356842	                     Acc@1 1.0 (0.984375
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][10/20]	                     loss 0.08878201246261597 (0.10352334049012926	                     Acc@1 1.0 (0.9861111111111112
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][11/20]	                     loss 0.09523941576480865 (0.1026949480175972	                     Acc@1 0.984375 (0.9859375
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][12/20]	                     loss 0.08467335999011993 (0.10105662183328108	                     Acc@1 1.0 (0.9872159090909091
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][13/20]	                     loss 0.08454837650060654 (0.09968093472222488	                     Acc@1 1.0 (0.98828125
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][14/20]	                     loss 0.0864553451538086 (0.09866358167850055	                     Acc@1 1.0 (0.9891826923076923
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][15/20]	                     loss 0.07450628280639648 (0.09693806033049311	                     Acc@1 1.0 (0.9899553571428571
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][16/20]	                     loss 0.08448994904756546 (0.0961081862449646	                     Acc@1 1.0 (0.990625
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][17/20]	                     loss 0.08871813118457794 (0.09564630780369043	                     Acc@1 0.9921875 (0.99072265625
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][18/20]	                     loss 0.08051014691591263 (0.09475594539852704	                     Acc@1 1.0 (0.9912683823529411
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][19/20]	                     loss 0.07592631876468658 (0.09370985502998035	                     Acc@1 1.0 (0.9917534722222222
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][20/20]	                     loss 0.08745458722114563 (0.09338063040846273	                     Acc@1 0.9921875 (0.9917763157894737
2024-11-18:12:47:50 [INFO    ] [ft-sam.py:312] Train: [15][21/20]	                     loss 0.09877506643533707 (0.09352735906839371	                     Acc@1 0.9852941176470589 (0.9916
2024-11-18:12:47:53 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.870206893329889,
 'clean_test_loss_avg_over_batch': 0.37276110996173906,
 'test_acc': 0.9038,
 'test_asr': 0.0011111111111111111,
 'test_ra': 0.5834444444444444,
 'train_acc': 0.9916,
 'train_epoch_loss_avg_over_batch': 0.09352735906839371}
2024-11-18:12:47:53 [INFO    ] [ft-sam.py:312] Train: [16][2/20]	                     loss 0.09250850975513458 (0.09250850975513458	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:53 [INFO    ] [ft-sam.py:312] Train: [16][3/20]	                     loss 0.11469785124063492 (0.10360318049788475	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:53 [INFO    ] [ft-sam.py:312] Train: [16][4/20]	                     loss 0.0875139981508255 (0.09824011971553166	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:53 [INFO    ] [ft-sam.py:312] Train: [16][5/20]	                     loss 0.09458787739276886 (0.09732705913484097	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][6/20]	                     loss 0.07063819468021393 (0.09198928624391556	                     Acc@1 1.0 (0.99375
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][7/20]	                     loss 0.10445722937583923 (0.09406727676590283	                     Acc@1 0.984375 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][8/20]	                     loss 0.08107881247997284 (0.09221178186791283	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][9/20]	                     loss 0.08605668693780899 (0.09144239500164986	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][10/20]	                     loss 0.08728258311748505 (0.0909801936811871	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][11/20]	                     loss 0.09407640993595123 (0.09128981530666351	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][12/20]	                     loss 0.0844850242137909 (0.09067119793458418	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][13/20]	                     loss 0.1039472371339798 (0.09177753453453381	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][14/20]	                     loss 0.09475549310445786 (0.09200660827068183	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][15/20]	                     loss 0.0781172513961792 (0.0910145113510745	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][16/20]	                     loss 0.11585298925638199 (0.092670409878095	                     Acc@1 0.984375 (0.9916666666666667
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][17/20]	                     loss 0.09754720330238342 (0.09297520946711302	                     Acc@1 0.984375 (0.9912109375
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][18/20]	                     loss 0.09096015244722366 (0.09285667670123718	                     Acc@1 0.9765625 (0.9903492647058824
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][19/20]	                     loss 0.08308102935552597 (0.09231358518203099	                     Acc@1 1.0 (0.9908854166666666
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][20/20]	                     loss 0.10231492668390274 (0.09283997157686635	                     Acc@1 0.9921875 (0.990953947368421
2024-11-18:12:47:54 [INFO    ] [ft-sam.py:312] Train: [16][21/20]	                     loss 0.08718899637460709 (0.0926862650513649	                     Acc@1 1.0 (0.9912
2024-11-18:12:47:57 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.922277846806486,
 'clean_test_loss_avg_over_batch': 0.3678078385470789,
 'test_acc': 0.9042,
 'test_asr': 0.001,
 'test_ra': 0.5818888888888889,
 'train_acc': 0.9912,
 'train_epoch_loss_avg_over_batch': 0.0926862650513649}
2024-11-18:12:47:57 [INFO    ] [ft-sam.py:312] Train: [17][2/20]	                     loss 0.08372865617275238 (0.08372865617275238	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][3/20]	                     loss 0.07542616873979568 (0.07957741245627403	                     Acc@1 1.0 (0.99609375
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][4/20]	                     loss 0.09918995201587677 (0.08611492564280827	                     Acc@1 0.984375 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][5/20]	                     loss 0.10333435237407684 (0.09041978232562542	                     Acc@1 0.984375 (0.990234375
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][6/20]	                     loss 0.09834694862365723 (0.09200521558523178	                     Acc@1 0.9921875 (0.990625
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][7/20]	                     loss 0.0885089561343193 (0.09142250567674637	                     Acc@1 0.9921875 (0.9908854166666666
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][8/20]	                     loss 0.07982295751571655 (0.08976542736802783	                     Acc@1 1.0 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][9/20]	                     loss 0.11347071826457977 (0.09272858873009682	                     Acc@1 0.984375 (0.9912109375
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][10/20]	                     loss 0.08552880585193634 (0.09192861285474566	                     Acc@1 1.0 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][11/20]	                     loss 0.08083318173885345 (0.09081906974315643	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][12/20]	                     loss 0.09768369793891907 (0.09144312685186212	                     Acc@1 1.0 (0.9928977272727273
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][13/20]	                     loss 0.09364868700504303 (0.09162692353129387	                     Acc@1 0.9921875 (0.9928385416666666
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][14/20]	                     loss 0.09618683159351349 (0.09197768568992615	                     Acc@1 0.9921875 (0.9927884615384616
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][15/20]	                     loss 0.08524760603904724 (0.09149696571486336	                     Acc@1 0.9921875 (0.9927455357142857
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][16/20]	                     loss 0.08889278024435043 (0.0913233533501625	                     Acc@1 1.0 (0.9932291666666667
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][17/20]	                     loss 0.08624738454818726 (0.09100610530003905	                     Acc@1 0.9921875 (0.9931640625
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][18/20]	                     loss 0.10600389540195465 (0.09188832824721056	                     Acc@1 0.9765625 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][19/20]	                     loss 0.08717368543148041 (0.09162640364633666	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][20/20]	                     loss 0.09453611075878143 (0.09177954612593901	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:47:58 [INFO    ] [ft-sam.py:312] Train: [17][21/20]	                     loss 0.09148796647787094 (0.09177161515951157	                     Acc@1 1.0 (0.9924
2024-11-18:12:48:01 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.78562549805977,
 'clean_test_loss_avg_over_batch': 0.3701573553719098,
 'test_acc': 0.9059,
 'test_asr': 0.0012222222222222222,
 'test_ra': 0.6058888888888889,
 'train_acc': 0.9924,
 'train_epoch_loss_avg_over_batch': 0.09177161515951157}
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][2/20]	                     loss 0.07753168046474457 (0.07753168046474457	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][3/20]	                     loss 0.09662306308746338 (0.08707737177610397	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][4/20]	                     loss 0.1150406002998352 (0.09639844795068105	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][5/20]	                     loss 0.10390333086252213 (0.09827466867864132	                     Acc@1 0.9921875 (0.9921875
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][6/20]	                     loss 0.08303070813417435 (0.09522587656974793	                     Acc@1 1.0 (0.99375
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][7/20]	                     loss 0.08688691258430481 (0.09383604923884074	                     Acc@1 1.0 (0.9947916666666666
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][8/20]	                     loss 0.07511264830827713 (0.09116127767733165	                     Acc@1 1.0 (0.9955357142857143
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][9/20]	                     loss 0.0777793824672699 (0.08948854077607393	                     Acc@1 1.0 (0.99609375
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][10/20]	                     loss 0.06898345053195953 (0.08721019741561678	                     Acc@1 1.0 (0.9965277777777778
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][11/20]	                     loss 0.07651915401220322 (0.08614109307527543	                     Acc@1 0.9921875 (0.99609375
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][12/20]	                     loss 0.08692062646150589 (0.08621195974675092	                     Acc@1 0.9921875 (0.9957386363636364
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][13/20]	                     loss 0.07028266042470932 (0.08488451813658078	                     Acc@1 1.0 (0.99609375
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][14/20]	                     loss 0.10332255065441132 (0.08630282833026005	                     Acc@1 0.984375 (0.9951923076923077
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][15/20]	                     loss 0.0867665708065033 (0.08633595279284886	                     Acc@1 0.9921875 (0.9949776785714286
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][16/20]	                     loss 0.09764156490564346 (0.08708966026703517	                     Acc@1 0.9921875 (0.9947916666666666
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][17/20]	                     loss 0.09370771050453186 (0.08750328840687871	                     Acc@1 0.9921875 (0.99462890625
2024-11-18:12:48:02 [INFO    ] [ft-sam.py:312] Train: [18][18/20]	                     loss 0.09089652448892593 (0.08770289052935208	                     Acc@1 0.9921875 (0.9944852941176471
2024-11-18:12:48:03 [INFO    ] [ft-sam.py:312] Train: [18][19/20]	                     loss 0.07768424600362778 (0.08714629916681184	                     Acc@1 1.0 (0.9947916666666666
2024-11-18:12:48:03 [INFO    ] [ft-sam.py:312] Train: [18][20/20]	                     loss 0.10174919664859772 (0.08791487271848478	                     Acc@1 0.9921875 (0.9946546052631579
2024-11-18:12:48:03 [INFO    ] [ft-sam.py:312] Train: [18][21/20]	                     loss 0.11292146146297455 (0.0885950519323349	                     Acc@1 0.9852941176470589 (0.9944
2024-11-18:12:48:05 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.635860127462467,
 'clean_test_loss_avg_over_batch': 0.3806560654428941,
 'test_acc': 0.9032,
 'test_asr': 0.0024444444444444444,
 'test_ra': 0.6405555555555555,
 'train_acc': 0.9944,
 'train_epoch_loss_avg_over_batch': 0.0885950519323349}
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][2/20]	                     loss 0.07796099781990051 (0.07796099781990051	                     Acc@1 1.0 (1.0
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][3/20]	                     loss 0.09437304735183716 (0.08616702258586884	                     Acc@1 0.9921875 (0.99609375
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][4/20]	                     loss 0.07141657918691635 (0.08125020811955135	                     Acc@1 1.0 (0.9973958333333334
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][5/20]	                     loss 0.09345635771751404 (0.08430174551904202	                     Acc@1 0.984375 (0.994140625
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][6/20]	                     loss 0.08247613906860352 (0.08393662422895432	                     Acc@1 1.0 (0.9953125
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][7/20]	                     loss 0.08965073525905609 (0.08488897606730461	                     Acc@1 0.9921875 (0.9947916666666666
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][8/20]	                     loss 0.09234314411878586 (0.08595385721751622	                     Acc@1 1.0 (0.9955357142857143
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][9/20]	                     loss 0.07452395558357239 (0.08452511951327324	                     Acc@1 1.0 (0.99609375
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][10/20]	                     loss 0.08759471774101257 (0.08486618598302205	                     Acc@1 0.9921875 (0.9956597222222222
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][11/20]	                     loss 0.07168333977460861 (0.08354790136218071	                     Acc@1 1.0 (0.99609375
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][12/20]	                     loss 0.08497326076030731 (0.08367747948928313	                     Acc@1 0.984375 (0.9950284090909091
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][13/20]	                     loss 0.08315718173980713 (0.08363412134349346	                     Acc@1 1.0 (0.9954427083333334
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][14/20]	                     loss 0.08522312343120575 (0.08375635227331749	                     Acc@1 0.9921875 (0.9951923076923077
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][15/20]	                     loss 0.09193121641874313 (0.08434027114084788	                     Acc@1 0.9921875 (0.9949776785714286
2024-11-18:12:48:06 [INFO    ] [ft-sam.py:312] Train: [19][16/20]	                     loss 0.0914173498749733 (0.08481207638978958	                     Acc@1 1.0 (0.9953125
2024-11-18:12:48:07 [INFO    ] [ft-sam.py:312] Train: [19][17/20]	                     loss 0.08871914446353912 (0.08505626814439893	                     Acc@1 0.9765625 (0.994140625
2024-11-18:12:48:07 [INFO    ] [ft-sam.py:312] Train: [19][18/20]	                     loss 0.11420522630214691 (0.08677091274191351	                     Acc@1 0.9765625 (0.9931066176470589
2024-11-18:12:48:07 [INFO    ] [ft-sam.py:312] Train: [19][19/20]	                     loss 0.08647652715444565 (0.08675455798705418	                     Acc@1 1.0 (0.9934895833333334
2024-11-18:12:48:07 [INFO    ] [ft-sam.py:312] Train: [19][20/20]	                     loss 0.0851481705904007 (0.08667001128196716	                     Acc@1 1.0 (0.9938322368421053
2024-11-18:12:48:07 [INFO    ] [ft-sam.py:312] Train: [19][21/20]	                     loss 0.10280651599168777 (0.08710892421007156	                     Acc@1 0.9852941176470589 (0.9936
2024-11-18:12:48:10 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 4.919730488683136,
 'clean_test_loss_avg_over_batch': 0.38027936974658244,
 'test_acc': 0.9045,
 'test_asr': 0.0012222222222222222,
 'test_ra': 0.5778888888888889,
 'train_acc': 0.9936,
 'train_epoch_loss_avg_over_batch': 0.08710892421007156}
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][2/20]	                     loss 0.07573141902685165 (0.07573141902685165	                     Acc@1 1.0 (1.0
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][3/20]	                     loss 0.0658392608165741 (0.07078533992171288	                     Acc@1 1.0 (1.0
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][4/20]	                     loss 0.09309475868940353 (0.07822181284427643	                     Acc@1 1.0 (1.0
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][5/20]	                     loss 0.06544281542301178 (0.07502706348896027	                     Acc@1 1.0 (1.0
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][6/20]	                     loss 0.0752280205488205 (0.07506725490093231	                     Acc@1 1.0 (1.0
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][7/20]	                     loss 0.08533050864934921 (0.07677779719233513	                     Acc@1 0.9921875 (0.9986979166666666
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][8/20]	                     loss 0.09213624894618988 (0.07897186172860009	                     Acc@1 0.9921875 (0.9977678571428571
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][9/20]	                     loss 0.09614960849285126 (0.08111908007413149	                     Acc@1 0.9765625 (0.9951171875
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][10/20]	                     loss 0.08068925142288208 (0.08107132133510378	                     Acc@1 1.0 (0.9956597222222222
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][11/20]	                     loss 0.08227138966321945 (0.08119132816791534	                     Acc@1 1.0 (0.99609375
2024-11-18:12:48:10 [INFO    ] [ft-sam.py:312] Train: [20][12/20]	                     loss 0.09312017261981964 (0.08227576857263391	                     Acc@1 0.9921875 (0.9957386363636364
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][13/20]	                     loss 0.08426976203918457 (0.08244193469484647	                     Acc@1 1.0 (0.99609375
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][14/20]	                     loss 0.08296696096658707 (0.08248232133113421	                     Acc@1 1.0 (0.9963942307692307
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][15/20]	                     loss 0.09113083779811859 (0.08310007250734738	                     Acc@1 0.9765625 (0.9949776785714286
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][16/20]	                     loss 0.08091533929109573 (0.08295442362626394	                     Acc@1 0.9921875 (0.9947916666666666
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][17/20]	                     loss 0.07006347924470901 (0.08214873960241675	                     Acc@1 1.0 (0.9951171875
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][18/20]	                     loss 0.08081544935703278 (0.082070310764453	                     Acc@1 0.9921875 (0.9949448529411765
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][19/20]	                     loss 0.06905625760555267 (0.08134730781118076	                     Acc@1 1.0 (0.9952256944444444
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][20/20]	                     loss 0.08050790429115295 (0.0813031286785477	                     Acc@1 0.984375 (0.9946546052631579
2024-11-18:12:48:11 [INFO    ] [ft-sam.py:312] Train: [20][21/20]	                     loss 0.12014393508434296 (0.08235959861278534	                     Acc@1 0.9705882352941176 (0.994
2024-11-18:12:48:14 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 5.196740929509552,
 'clean_test_loss_avg_over_batch': 0.3944412152978438,
 'test_acc': 0.9014,
 'test_asr': 0.0005555555555555556,
 'test_ra': 0.5896666666666667,
 'train_acc': 0.994,
 'train_epoch_loss_avg_over_batch': 0.08235959861278534}
2024-11-18:12:48:14 [INFO    ] [save_load_attack.py:176] saving...
