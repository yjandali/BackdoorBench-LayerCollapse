2024-11-17:16:53:39 [INFO    ] [ft-sam.py:234] {'ExponentialLR_gamma': 0.9,
 'adaptive': True,
 'alpha': 0.0,
 'amp': True,
 'batch_size': 128,
 'checkpoint_load': None,
 'checkpoint_save': 'record/sig_0_1_ViT/defense/ft-sam/checkpoint/',
 'client_optimizer': 'AdamW',
 'dataset': 'cifar10',
 'dataset_path': './data/cifar10',
 'device': 'cuda:0',
 'epochs': 20,
 'frequency_save': 100,
 'img_size': (32, 32, 3),
 'index': None,
 'input_channel': 3,
 'input_height': 32,
 'input_width': 32,
 'label_smoothing': 0.1,
 'log': 'record/sig_0_1_ViT/defense/ft-sam/log/',
 'lr': 1e-05,
 'lr_scheduler': 'CosineAnnealingLR',
 'model': 'vit_b_16',
 'non_blocking': True,
 'num_classes': 10,
 'num_workers': 4,
 'pin_memory': True,
 'prefetch': False,
 'print_freq': 1,
 'random_seed': 0,
 'ratio': 0.05,
 'result_file': 'sig_0_1_ViT',
 'rho': 2.0,
 'rho_max': 2.0,
 'rho_min': 2.0,
 'save_path': 'record/sig_0_1_ViT/defense/ft-sam/',
 'sgd_momentum': 0.99,
 'terminal_info': ['./defense/ft-sam.py',
                   '--result_file',
                   'sig_0_1_ViT',
                   '--yaml_path',
                   './config-vit/defense/ft-sam/cifar10.yaml',
                   '--dataset',
                   'cifar10',
                   '--epochs',
                   '20',
                   '--ratio',
                   '0.05',
                   '--device',
                   'cuda:0',
                   '--model',
                   'vit_b_16'],
 'wd': 0.0005,
 'yaml_path': './config-vit/defense/ft-sam/cifar10.yaml'}
2024-11-17:16:53:41 [INFO    ] [bd_dataset_v2.py:133] save file format is .png
2024-11-17:16:53:45 [INFO    ] [ft-sam.py:312] Train: [1][2/20]	                     loss 1.4955036640167236 (1.4955036640167236	                     Acc@1 0.875 (0.875
2024-11-17:16:53:47 [INFO    ] [ft-sam.py:312] Train: [1][3/20]	                     loss 1.2646716833114624 (1.380087673664093	                     Acc@1 0.90625 (0.890625
2024-11-17:16:53:49 [INFO    ] [ft-sam.py:312] Train: [1][4/20]	                     loss 1.3167080879211426 (1.3589611450831096	                     Acc@1 0.8671875 (0.8828125
2024-11-17:16:53:51 [INFO    ] [ft-sam.py:312] Train: [1][5/20]	                     loss 1.149549961090088 (1.3066083490848541	                     Acc@1 0.8984375 (0.88671875
2024-11-17:16:53:53 [INFO    ] [ft-sam.py:312] Train: [1][6/20]	                     loss 1.2676876783370972 (1.2988242149353026	                     Acc@1 0.8828125 (0.8859375
2024-11-17:16:53:55 [INFO    ] [ft-sam.py:312] Train: [1][7/20]	                     loss 1.215773344039917 (1.284982403119405	                     Acc@1 0.8984375 (0.8880208333333334
2024-11-17:16:53:57 [INFO    ] [ft-sam.py:312] Train: [1][8/20]	                     loss 1.0820610523223877 (1.2559936387198312	                     Acc@1 0.8828125 (0.8872767857142857
2024-11-17:16:53:59 [INFO    ] [ft-sam.py:312] Train: [1][9/20]	                     loss 0.8799922466278076 (1.2089934647083282	                     Acc@1 0.9375 (0.8935546875
2024-11-17:16:54:01 [INFO    ] [ft-sam.py:312] Train: [1][10/20]	                     loss 0.98225998878479 (1.1838008562723796	                     Acc@1 0.90625 (0.8949652777777778
2024-11-17:16:54:03 [INFO    ] [ft-sam.py:312] Train: [1][11/20]	                     loss 0.9947409629821777 (1.1648948669433594	                     Acc@1 0.8828125 (0.89375
2024-11-17:16:54:05 [INFO    ] [ft-sam.py:312] Train: [1][12/20]	                     loss 0.911748468875885 (1.1418815580281345	                     Acc@1 0.921875 (0.8963068181818182
2024-11-17:16:54:07 [INFO    ] [ft-sam.py:312] Train: [1][13/20]	                     loss 0.9644105434417725 (1.1270923068126042	                     Acc@1 0.8984375 (0.896484375
2024-11-17:16:54:09 [INFO    ] [ft-sam.py:312] Train: [1][14/20]	                     loss 1.0476707220077515 (1.1209829541353078	                     Acc@1 0.859375 (0.8936298076923077
2024-11-17:16:54:11 [INFO    ] [ft-sam.py:312] Train: [1][15/20]	                     loss 0.7142871618270874 (1.0919332546847207	                     Acc@1 0.9453125 (0.8973214285714286
2024-11-17:16:54:13 [INFO    ] [ft-sam.py:312] Train: [1][16/20]	                     loss 0.6567928194999695 (1.0629238923390707	                     Acc@1 0.953125 (0.9010416666666666
2024-11-17:16:54:15 [INFO    ] [ft-sam.py:312] Train: [1][17/20]	                     loss 0.6737867593765259 (1.0386028215289116	                     Acc@1 0.9453125 (0.90380859375
2024-11-17:16:54:17 [INFO    ] [ft-sam.py:312] Train: [1][18/20]	                     loss 0.6415083408355713 (1.0152443226645975	                     Acc@1 0.9453125 (0.90625
2024-11-17:16:54:19 [INFO    ] [ft-sam.py:312] Train: [1][19/20]	                     loss 0.7267594933509827 (0.9992173877027299	                     Acc@1 0.90625 (0.90625
2024-11-17:16:54:21 [INFO    ] [ft-sam.py:312] Train: [1][20/20]	                     loss 0.678999125957489 (0.9823637949792963	                     Acc@1 0.921875 (0.9070723684210527
2024-11-17:16:54:22 [INFO    ] [ft-sam.py:312] Train: [1][21/20]	                     loss 0.5634744763374329 (0.9709700055122376	                     Acc@1 0.9558823529411765 (0.9084
2024-11-17:16:55:31 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.027434154498082598,
 'clean_test_loss_avg_over_batch': 0.24609443422737,
 'test_acc': 0.922,
 'test_asr': 0.9904444444444445,
 'test_ra': 0.009333333333333334,
 'train_acc': 0.9084,
 'train_epoch_loss_avg_over_batch': 0.9709700055122376}
2024-11-17:16:55:33 [INFO    ] [ft-sam.py:312] Train: [2][2/20]	                     loss 0.6152433156967163 (0.6152433156967163	                     Acc@1 0.9296875 (0.9296875
2024-11-17:16:55:35 [INFO    ] [ft-sam.py:312] Train: [2][3/20]	                     loss 0.4719996750354767 (0.5436214953660965	                     Acc@1 0.96875 (0.94921875
2024-11-17:16:55:37 [INFO    ] [ft-sam.py:312] Train: [2][4/20]	                     loss 0.5086429119110107 (0.5319619675477346	                     Acc@1 0.953125 (0.9505208333333334
2024-11-17:16:55:39 [INFO    ] [ft-sam.py:312] Train: [2][5/20]	                     loss 0.5308510065078735 (0.5316842272877693	                     Acc@1 0.9375 (0.947265625
2024-11-17:16:55:41 [INFO    ] [ft-sam.py:312] Train: [2][6/20]	                     loss 0.47631001472473145 (0.5206093847751617	                     Acc@1 0.9609375 (0.95
2024-11-17:16:55:43 [INFO    ] [ft-sam.py:312] Train: [2][7/20]	                     loss 0.4554010033607483 (0.5097413212060928	                     Acc@1 0.9609375 (0.9518229166666666
2024-11-17:16:55:45 [INFO    ] [ft-sam.py:312] Train: [2][8/20]	                     loss 0.46396440267562866 (0.5032017614160266	                     Acc@1 0.96875 (0.9542410714285714
2024-11-17:16:55:47 [INFO    ] [ft-sam.py:312] Train: [2][9/20]	                     loss 0.4591973125934601 (0.4977012053132057	                     Acc@1 0.96875 (0.9560546875
2024-11-17:16:55:49 [INFO    ] [ft-sam.py:312] Train: [2][10/20]	                     loss 0.4405651092529297 (0.49135275019539726	                     Acc@1 0.984375 (0.9592013888888888
2024-11-17:16:55:51 [INFO    ] [ft-sam.py:312] Train: [2][11/20]	                     loss 0.4257238805294037 (0.4847898632287979	                     Acc@1 0.96875 (0.96015625
2024-11-17:16:55:53 [INFO    ] [ft-sam.py:312] Train: [2][12/20]	                     loss 0.4502515494823456 (0.48165001652457495	                     Acc@1 0.9296875 (0.9573863636363636
2024-11-17:16:55:55 [INFO    ] [ft-sam.py:312] Train: [2][13/20]	                     loss 0.39500823616981506 (0.4744298681616783	                     Acc@1 0.96875 (0.9583333333333334
2024-11-17:16:55:57 [INFO    ] [ft-sam.py:312] Train: [2][14/20]	                     loss 0.3985936939716339 (0.46859631630090565	                     Acc@1 0.9609375 (0.9585336538461539
2024-11-17:16:55:59 [INFO    ] [ft-sam.py:312] Train: [2][15/20]	                     loss 0.36513757705688477 (0.4612064063549042	                     Acc@1 0.96875 (0.9592633928571429
2024-11-17:16:56:01 [INFO    ] [ft-sam.py:312] Train: [2][16/20]	                     loss 0.3783413767814636 (0.45568207105000813	                     Acc@1 0.9765625 (0.9604166666666667
2024-11-17:16:56:03 [INFO    ] [ft-sam.py:312] Train: [2][17/20]	                     loss 0.37190547585487366 (0.45044603385031223	                     Acc@1 0.9765625 (0.96142578125
2024-11-17:16:56:05 [INFO    ] [ft-sam.py:312] Train: [2][18/20]	                     loss 0.41280096769332886 (0.4482316181940191	                     Acc@1 0.96875 (0.9618566176470589
2024-11-17:16:56:07 [INFO    ] [ft-sam.py:312] Train: [2][19/20]	                     loss 0.393058717250824 (0.4451664570305083	                     Acc@1 0.9609375 (0.9618055555555556
2024-11-17:16:56:09 [INFO    ] [ft-sam.py:312] Train: [2][20/20]	                     loss 0.35120946168899536 (0.4402213520125339	                     Acc@1 0.96875 (0.962171052631579
2024-11-17:16:56:10 [INFO    ] [ft-sam.py:312] Train: [2][21/20]	                     loss 0.3396778702735901 (0.43748656930923463	                     Acc@1 0.9705882352941176 (0.9624
2024-11-17:16:57:20 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.06406903863084358,
 'clean_test_loss_avg_over_batch': 0.21683788290129433,
 'test_acc': 0.9307,
 'test_asr': 0.9761111111111112,
 'test_ra': 0.023666666666666666,
 'train_acc': 0.9624,
 'train_epoch_loss_avg_over_batch': 0.43748656930923463}
2024-11-17:16:57:22 [INFO    ] [ft-sam.py:312] Train: [3][2/20]	                     loss 0.32050347328186035 (0.32050347328186035	                     Acc@1 0.984375 (0.984375
2024-11-17:16:57:24 [INFO    ] [ft-sam.py:312] Train: [3][3/20]	                     loss 0.38749372959136963 (0.353998601436615	                     Acc@1 0.921875 (0.953125
2024-11-17:16:57:26 [INFO    ] [ft-sam.py:312] Train: [3][4/20]	                     loss 0.368478387594223 (0.3588251968224843	                     Acc@1 0.9765625 (0.9609375
2024-11-17:16:57:28 [INFO    ] [ft-sam.py:312] Train: [3][5/20]	                     loss 0.35000845789909363 (0.35662101209163666	                     Acc@1 0.953125 (0.958984375
2024-11-17:16:57:30 [INFO    ] [ft-sam.py:312] Train: [3][6/20]	                     loss 0.33135563135147095 (0.3515679359436035	                     Acc@1 0.953125 (0.9578125
2024-11-17:16:57:32 [INFO    ] [ft-sam.py:312] Train: [3][7/20]	                     loss 0.33042752742767334 (0.3480445345242818	                     Acc@1 0.953125 (0.95703125
2024-11-17:16:57:34 [INFO    ] [ft-sam.py:312] Train: [3][8/20]	                     loss 0.34065425395965576 (0.3469887801579067	                     Acc@1 0.96875 (0.9587053571428571
2024-11-17:16:57:36 [INFO    ] [ft-sam.py:312] Train: [3][9/20]	                     loss 0.3402564823627472 (0.34614724293351173	                     Acc@1 0.9609375 (0.958984375
2024-11-17:16:57:38 [INFO    ] [ft-sam.py:312] Train: [3][10/20]	                     loss 0.3021903336048126 (0.3412631418969896	                     Acc@1 0.9609375 (0.9592013888888888
2024-11-17:16:57:40 [INFO    ] [ft-sam.py:312] Train: [3][11/20]	                     loss 0.30548545718193054 (0.3376853734254837	                     Acc@1 0.96875 (0.96015625
2024-11-17:16:57:42 [INFO    ] [ft-sam.py:312] Train: [3][12/20]	                     loss 0.32784226536750793 (0.3367905454202132	                     Acc@1 0.953125 (0.9595170454545454
2024-11-17:16:57:44 [INFO    ] [ft-sam.py:312] Train: [3][13/20]	                     loss 0.2913837730884552 (0.3330066477259	                     Acc@1 0.96875 (0.9602864583333334
2024-11-17:16:57:46 [INFO    ] [ft-sam.py:312] Train: [3][14/20]	                     loss 0.34783631563186646 (0.3341473914109744	                     Acc@1 0.9375 (0.9585336538461539
2024-11-17:16:57:48 [INFO    ] [ft-sam.py:312] Train: [3][15/20]	                     loss 0.2996431291103363 (0.33168280124664307	                     Acc@1 0.96875 (0.9592633928571429
2024-11-17:16:57:50 [INFO    ] [ft-sam.py:312] Train: [3][16/20]	                     loss 0.3163265287876129 (0.3306590497493744	                     Acc@1 0.96875 (0.9598958333333333
2024-11-17:16:57:52 [INFO    ] [ft-sam.py:312] Train: [3][17/20]	                     loss 0.27942779660224915 (0.32745709642767906	                     Acc@1 0.9765625 (0.9609375
2024-11-17:16:57:53 [INFO    ] [ft-sam.py:312] Train: [3][18/20]	                     loss 0.3218255937099457 (0.32712583156193004	                     Acc@1 0.953125 (0.9604779411764706
2024-11-17:16:57:55 [INFO    ] [ft-sam.py:312] Train: [3][19/20]	                     loss 0.29810717701911926 (0.32551368408732945	                     Acc@1 0.9375 (0.9592013888888888
2024-11-17:16:57:57 [INFO    ] [ft-sam.py:312] Train: [3][20/20]	                     loss 0.2844197154045105 (0.32335084363033895	                     Acc@1 0.984375 (0.9605263157894737
2024-11-17:16:57:58 [INFO    ] [ft-sam.py:312] Train: [3][21/20]	                     loss 0.2628763020038605 (0.3217059360980988	                     Acc@1 0.9852941176470589 (0.9612
2024-11-17:16:59:08 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.08259472712664537,
 'clean_test_loss_avg_over_batch': 0.2525472021366976,
 'test_acc': 0.9269,
 'test_asr': 0.9677777777777777,
 'test_ra': 0.032,
 'train_acc': 0.9612,
 'train_epoch_loss_avg_over_batch': 0.3217059360980988}
2024-11-17:16:59:10 [INFO    ] [ft-sam.py:312] Train: [4][2/20]	                     loss 0.322725772857666 (0.322725772857666	                     Acc@1 0.9609375 (0.9609375
2024-11-17:16:59:12 [INFO    ] [ft-sam.py:312] Train: [4][3/20]	                     loss 0.2763465642929077 (0.29953616857528687	                     Acc@1 0.96875 (0.96484375
2024-11-17:16:59:14 [INFO    ] [ft-sam.py:312] Train: [4][4/20]	                     loss 0.30195069313049316 (0.30034101009368896	                     Acc@1 0.953125 (0.9609375
2024-11-17:16:59:16 [INFO    ] [ft-sam.py:312] Train: [4][5/20]	                     loss 0.30279701948165894 (0.30095501244068146	                     Acc@1 0.953125 (0.958984375
2024-11-17:16:59:18 [INFO    ] [ft-sam.py:312] Train: [4][6/20]	                     loss 0.2794639468193054 (0.29665679931640626	                     Acc@1 0.9765625 (0.9625
2024-11-17:16:59:20 [INFO    ] [ft-sam.py:312] Train: [4][7/20]	                     loss 0.293882817029953 (0.2961944689353307	                     Acc@1 0.9765625 (0.96484375
2024-11-17:16:59:22 [INFO    ] [ft-sam.py:312] Train: [4][8/20]	                     loss 0.2929980158805847 (0.2957378327846527	                     Acc@1 0.9609375 (0.9642857142857143
2024-11-17:16:59:24 [INFO    ] [ft-sam.py:312] Train: [4][9/20]	                     loss 0.26440849900245667 (0.2918216660618782	                     Acc@1 0.96875 (0.96484375
2024-11-17:16:59:26 [INFO    ] [ft-sam.py:312] Train: [4][10/20]	                     loss 0.28433531522750854 (0.2909898493025038	                     Acc@1 0.9453125 (0.9626736111111112
2024-11-17:16:59:28 [INFO    ] [ft-sam.py:312] Train: [4][11/20]	                     loss 0.25729912519454956 (0.2876207768917084	                     Acc@1 0.984375 (0.96484375
2024-11-17:16:59:30 [INFO    ] [ft-sam.py:312] Train: [4][12/20]	                     loss 0.261059433221817 (0.2852061092853546	                     Acc@1 0.9765625 (0.9659090909090909
2024-11-17:16:59:32 [INFO    ] [ft-sam.py:312] Train: [4][13/20]	                     loss 0.2643026113510132 (0.2834641511241595	                     Acc@1 0.953125 (0.96484375
2024-11-17:16:59:34 [INFO    ] [ft-sam.py:312] Train: [4][14/20]	                     loss 0.24407416582107544 (0.2804341522546915	                     Acc@1 0.96875 (0.9651442307692307
2024-11-17:16:59:36 [INFO    ] [ft-sam.py:312] Train: [4][15/20]	                     loss 0.23671942949295044 (0.2773116720574243	                     Acc@1 0.984375 (0.9665178571428571
2024-11-17:16:59:38 [INFO    ] [ft-sam.py:312] Train: [4][16/20]	                     loss 0.25942859053611755 (0.2761194666226705	                     Acc@1 0.9609375 (0.9661458333333334
2024-11-17:16:59:40 [INFO    ] [ft-sam.py:312] Train: [4][17/20]	                     loss 0.25755032896995544 (0.2749588955193758	                     Acc@1 0.9609375 (0.9658203125
2024-11-17:16:59:42 [INFO    ] [ft-sam.py:312] Train: [4][18/20]	                     loss 0.2761417031288147 (0.2750284724375781	                     Acc@1 0.9609375 (0.9655330882352942
2024-11-17:16:59:44 [INFO    ] [ft-sam.py:312] Train: [4][19/20]	                     loss 0.26698213815689087 (0.2745814538664288	                     Acc@1 0.953125 (0.96484375
2024-11-17:16:59:46 [INFO    ] [ft-sam.py:312] Train: [4][20/20]	                     loss 0.25814199447631836 (0.27371621916168615	                     Acc@1 0.96875 (0.9650493421052632
2024-11-17:16:59:47 [INFO    ] [ft-sam.py:312] Train: [4][21/20]	                     loss 0.26813769340515137 (0.2735644832611084	                     Acc@1 0.9411764705882353 (0.9644
2024-11-17:17:00:56 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.09647315541203592,
 'clean_test_loss_avg_over_batch': 0.25827990038485465,
 'test_acc': 0.927,
 'test_asr': 0.9624444444444444,
 'test_ra': 0.03722222222222222,
 'train_acc': 0.9644,
 'train_epoch_loss_avg_over_batch': 0.2735644832611084}
2024-11-17:17:00:59 [INFO    ] [ft-sam.py:312] Train: [5][2/20]	                     loss 0.25987350940704346 (0.25987350940704346	                     Acc@1 0.9765625 (0.9765625
2024-11-17:17:01:01 [INFO    ] [ft-sam.py:312] Train: [5][3/20]	                     loss 0.2671695053577423 (0.2635215073823929	                     Acc@1 0.984375 (0.98046875
2024-11-17:17:01:03 [INFO    ] [ft-sam.py:312] Train: [5][4/20]	                     loss 0.2853408455848694 (0.2707946201165517	                     Acc@1 0.9609375 (0.9739583333333334
2024-11-17:17:01:05 [INFO    ] [ft-sam.py:312] Train: [5][5/20]	                     loss 0.26915961503982544 (0.27038586884737015	                     Acc@1 0.9453125 (0.966796875
2024-11-17:17:01:06 [INFO    ] [ft-sam.py:312] Train: [5][6/20]	                     loss 0.2358245998620987 (0.26347361505031586	                     Acc@1 0.9765625 (0.96875
2024-11-17:17:01:08 [INFO    ] [ft-sam.py:312] Train: [5][7/20]	                     loss 0.29255786538124084 (0.26832099010547	                     Acc@1 0.9453125 (0.96484375
2024-11-17:17:01:10 [INFO    ] [ft-sam.py:312] Train: [5][8/20]	                     loss 0.2662922143936157 (0.26803116500377655	                     Acc@1 0.9609375 (0.9642857142857143
2024-11-17:17:01:12 [INFO    ] [ft-sam.py:312] Train: [5][9/20]	                     loss 0.27561458945274353 (0.2689790930598974	                     Acc@1 0.953125 (0.962890625
2024-11-17:17:01:14 [INFO    ] [ft-sam.py:312] Train: [5][10/20]	                     loss 0.2527841031551361 (0.26717964973714614	                     Acc@1 0.9765625 (0.9644097222222222
2024-11-17:17:01:16 [INFO    ] [ft-sam.py:312] Train: [5][11/20]	                     loss 0.2750006914138794 (0.26796175390481947	                     Acc@1 0.9609375 (0.9640625
2024-11-17:17:01:18 [INFO    ] [ft-sam.py:312] Train: [5][12/20]	                     loss 0.2201213538646698 (0.26361262662844226	                     Acc@1 0.9921875 (0.9666193181818182
2024-11-17:17:01:20 [INFO    ] [ft-sam.py:312] Train: [5][13/20]	                     loss 0.2589593231678009 (0.26322485134005547	                     Acc@1 0.984375 (0.9680989583333334
2024-11-17:17:01:22 [INFO    ] [ft-sam.py:312] Train: [5][14/20]	                     loss 0.2808957099914551 (0.2645841481593939	                     Acc@1 0.953125 (0.9669471153846154
2024-11-17:17:01:24 [INFO    ] [ft-sam.py:312] Train: [5][15/20]	                     loss 0.24609369039535522 (0.2632634011762483	                     Acc@1 0.96875 (0.9670758928571429
2024-11-17:17:01:26 [INFO    ] [ft-sam.py:312] Train: [5][16/20]	                     loss 0.26447761058807373 (0.26334434847036997	                     Acc@1 0.9609375 (0.9666666666666667
2024-11-17:17:01:28 [INFO    ] [ft-sam.py:312] Train: [5][17/20]	                     loss 0.28780293464660645 (0.26487301010638475	                     Acc@1 0.953125 (0.9658203125
2024-11-17:17:01:30 [INFO    ] [ft-sam.py:312] Train: [5][18/20]	                     loss 0.27008309960365295 (0.26517948595916524	                     Acc@1 0.96875 (0.9659926470588235
2024-11-17:17:01:32 [INFO    ] [ft-sam.py:312] Train: [5][19/20]	                     loss 0.27685242891311646 (0.2658279827899403	                     Acc@1 0.9453125 (0.96484375
2024-11-17:17:01:34 [INFO    ] [ft-sam.py:312] Train: [5][20/20]	                     loss 0.24202075600624084 (0.26457497085395615	                     Acc@1 0.96875 (0.9650493421052632
2024-11-17:17:01:35 [INFO    ] [ft-sam.py:312] Train: [5][21/20]	                     loss 0.2531728744506836 (0.2642648338317871	                     Acc@1 0.9852941176470589 (0.9656
2024-11-17:17:02:45 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.0984571802259331,
 'clean_test_loss_avg_over_batch': 0.25998986314369155,
 'test_acc': 0.9271,
 'test_asr': 0.9617777777777777,
 'test_ra': 0.03788888888888889,
 'train_acc': 0.9656,
 'train_epoch_loss_avg_over_batch': 0.2642648338317871}
2024-11-17:17:02:47 [INFO    ] [ft-sam.py:312] Train: [6][2/20]	                     loss 0.2862861156463623 (0.2862861156463623	                     Acc@1 0.953125 (0.953125
2024-11-17:17:02:49 [INFO    ] [ft-sam.py:312] Train: [6][3/20]	                     loss 0.2349075973033905 (0.2605968564748764	                     Acc@1 0.9921875 (0.97265625
2024-11-17:17:02:51 [INFO    ] [ft-sam.py:312] Train: [6][4/20]	                     loss 0.2669455111026764 (0.26271307468414307	                     Acc@1 0.96875 (0.9713541666666666
2024-11-17:17:02:53 [INFO    ] [ft-sam.py:312] Train: [6][5/20]	                     loss 0.23872891068458557 (0.2567170336842537	                     Acc@1 0.96875 (0.970703125
2024-11-17:17:02:55 [INFO    ] [ft-sam.py:312] Train: [6][6/20]	                     loss 0.28119879961013794 (0.26161338686943053	                     Acc@1 0.9296875 (0.9625
2024-11-17:17:02:57 [INFO    ] [ft-sam.py:312] Train: [6][7/20]	                     loss 0.27667564153671265 (0.2641237626473109	                     Acc@1 0.9609375 (0.9622395833333334
2024-11-17:17:02:59 [INFO    ] [ft-sam.py:312] Train: [6][8/20]	                     loss 0.23535093665122986 (0.260013358933585	                     Acc@1 0.9765625 (0.9642857142857143
2024-11-17:17:03:01 [INFO    ] [ft-sam.py:312] Train: [6][9/20]	                     loss 0.2694036364555359 (0.2611871436238289	                     Acc@1 0.953125 (0.962890625
2024-11-17:17:03:03 [INFO    ] [ft-sam.py:312] Train: [6][10/20]	                     loss 0.254556804895401 (0.26045043932067025	                     Acc@1 0.96875 (0.9635416666666666
2024-11-17:17:03:05 [INFO    ] [ft-sam.py:312] Train: [6][11/20]	                     loss 0.22338691353797913 (0.2567440867424011	                     Acc@1 0.9765625 (0.96484375
2024-11-17:17:03:07 [INFO    ] [ft-sam.py:312] Train: [6][12/20]	                     loss 0.28067103028297424 (0.2589192634279078	                     Acc@1 0.9375 (0.9623579545454546
2024-11-17:17:03:09 [INFO    ] [ft-sam.py:312] Train: [6][13/20]	                     loss 0.2769327163696289 (0.26042038450638455	                     Acc@1 0.9765625 (0.9635416666666666
2024-11-17:17:03:11 [INFO    ] [ft-sam.py:312] Train: [6][14/20]	                     loss 0.2919020652770996 (0.262842052257978	                     Acc@1 0.9609375 (0.9633413461538461
2024-11-17:17:03:13 [INFO    ] [ft-sam.py:312] Train: [6][15/20]	                     loss 0.24265342950820923 (0.26140000777585165	                     Acc@1 0.9765625 (0.9642857142857143
2024-11-17:17:03:14 [INFO    ] [ft-sam.py:312] Train: [6][16/20]	                     loss 0.2556517720222473 (0.2610167920589447	                     Acc@1 0.9609375 (0.9640625
2024-11-17:17:03:16 [INFO    ] [ft-sam.py:312] Train: [6][17/20]	                     loss 0.23796263337135315 (0.25957590714097023	                     Acc@1 0.9765625 (0.96484375
2024-11-17:17:03:18 [INFO    ] [ft-sam.py:312] Train: [6][18/20]	                     loss 0.2725294828414917 (0.26033788218217735	                     Acc@1 0.9765625 (0.9655330882352942
2024-11-17:17:03:20 [INFO    ] [ft-sam.py:312] Train: [6][19/20]	                     loss 0.23053237795829773 (0.2586820208364063	                     Acc@1 0.9921875 (0.9670138888888888
2024-11-17:17:03:22 [INFO    ] [ft-sam.py:312] Train: [6][20/20]	                     loss 0.26550430059432983 (0.2590410881920865	                     Acc@1 0.9375 (0.9654605263157895
2024-11-17:17:03:23 [INFO    ] [ft-sam.py:312] Train: [6][21/20]	                     loss 0.2686307728290558 (0.25930192761421206	                     Acc@1 0.9705882352941176 (0.9656
2024-11-17:17:04:33 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.098275651132137,
 'clean_test_loss_avg_over_batch': 0.2627324963672252,
 'test_acc': 0.9269,
 'test_asr': 0.9621111111111111,
 'test_ra': 0.03755555555555556,
 'train_acc': 0.9656,
 'train_epoch_loss_avg_over_batch': 0.25930192761421206}
2024-11-17:17:04:35 [INFO    ] [ft-sam.py:312] Train: [7][2/20]	                     loss 0.296064168214798 (0.296064168214798	                     Acc@1 0.9453125 (0.9453125
2024-11-17:17:04:37 [INFO    ] [ft-sam.py:312] Train: [7][3/20]	                     loss 0.2439252883195877 (0.26999472826719284	                     Acc@1 0.9921875 (0.96875
2024-11-17:17:04:39 [INFO    ] [ft-sam.py:312] Train: [7][4/20]	                     loss 0.25904297828674316 (0.2663441449403763	                     Acc@1 0.96875 (0.96875
2024-11-17:17:04:41 [INFO    ] [ft-sam.py:312] Train: [7][5/20]	                     loss 0.2682841122150421 (0.26682913675904274	                     Acc@1 0.953125 (0.96484375
2024-11-17:17:04:43 [INFO    ] [ft-sam.py:312] Train: [7][6/20]	                     loss 0.24451595544815063 (0.26236650049686433	                     Acc@1 0.9765625 (0.9671875
2024-11-17:17:04:45 [INFO    ] [ft-sam.py:312] Train: [7][7/20]	                     loss 0.2684648633003235 (0.26338289429744083	                     Acc@1 0.9609375 (0.9661458333333334
2024-11-17:17:04:47 [INFO    ] [ft-sam.py:312] Train: [7][8/20]	                     loss 0.2540897727012634 (0.26205530549798695	                     Acc@1 0.953125 (0.9642857142857143
2024-11-17:17:04:49 [INFO    ] [ft-sam.py:312] Train: [7][9/20]	                     loss 0.2625149190425873 (0.262112757191062	                     Acc@1 0.9765625 (0.9658203125
2024-11-17:17:04:51 [INFO    ] [ft-sam.py:312] Train: [7][10/20]	                     loss 0.24365156888961792 (0.26006151404645705	                     Acc@1 0.9765625 (0.9670138888888888
2024-11-17:17:04:53 [INFO    ] [ft-sam.py:312] Train: [7][11/20]	                     loss 0.2650388479232788 (0.2605592474341393	                     Acc@1 0.96875 (0.9671875
2024-11-17:17:04:55 [INFO    ] [ft-sam.py:312] Train: [7][12/20]	                     loss 0.2666833996772766 (0.26111598854715173	                     Acc@1 0.9453125 (0.9651988636363636
2024-11-17:17:04:57 [INFO    ] [ft-sam.py:312] Train: [7][13/20]	                     loss 0.2603580355644226 (0.26105282579859096	                     Acc@1 0.9609375 (0.96484375
2024-11-17:17:04:59 [INFO    ] [ft-sam.py:312] Train: [7][14/20]	                     loss 0.2489922195672989 (0.26012508685772234	                     Acc@1 0.96875 (0.9651442307692307
2024-11-17:17:05:01 [INFO    ] [ft-sam.py:312] Train: [7][15/20]	                     loss 0.2641495168209076 (0.260412546140807	                     Acc@1 0.96875 (0.9654017857142857
2024-11-17:17:05:03 [INFO    ] [ft-sam.py:312] Train: [7][16/20]	                     loss 0.27008992433547974 (0.26105770468711853	                     Acc@1 0.9609375 (0.9651041666666667
2024-11-17:17:05:05 [INFO    ] [ft-sam.py:312] Train: [7][17/20]	                     loss 0.2448503077030182 (0.26004474237561226	                     Acc@1 0.984375 (0.96630859375
2024-11-17:17:05:07 [INFO    ] [ft-sam.py:312] Train: [7][18/20]	                     loss 0.3034597635269165 (0.26259856714921836	                     Acc@1 0.9296875 (0.9641544117647058
2024-11-17:17:05:09 [INFO    ] [ft-sam.py:312] Train: [7][19/20]	                     loss 0.2647382318973541 (0.2627174374130037	                     Acc@1 0.9375 (0.9626736111111112
2024-11-17:17:05:11 [INFO    ] [ft-sam.py:312] Train: [7][20/20]	                     loss 0.22603529691696167 (0.2607867984395278	                     Acc@1 0.96875 (0.9629934210526315
2024-11-17:17:05:12 [INFO    ] [ft-sam.py:312] Train: [7][21/20]	                     loss 0.2564675509929657 (0.2606693149089813	                     Acc@1 0.9411764705882353 (0.9624
2024-11-17:17:06:21 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.10065273568034172,
 'clean_test_loss_avg_over_batch': 0.27253357537939576,
 'test_acc': 0.9269,
 'test_asr': 0.9608888888888889,
 'test_ra': 0.03866666666666667,
 'train_acc': 0.9624,
 'train_epoch_loss_avg_over_batch': 0.2606693149089813}
2024-11-17:17:06:24 [INFO    ] [ft-sam.py:312] Train: [8][2/20]	                     loss 0.2048167586326599 (0.2048167586326599	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:06:26 [INFO    ] [ft-sam.py:312] Train: [8][3/20]	                     loss 0.25810396671295166 (0.2314603626728058	                     Acc@1 0.984375 (0.98828125
2024-11-17:17:06:28 [INFO    ] [ft-sam.py:312] Train: [8][4/20]	                     loss 0.24979764223098755 (0.2375727891921997	                     Acc@1 0.96875 (0.9817708333333334
2024-11-17:17:06:29 [INFO    ] [ft-sam.py:312] Train: [8][5/20]	                     loss 0.24461594223976135 (0.23933357745409012	                     Acc@1 0.984375 (0.982421875
2024-11-17:17:06:31 [INFO    ] [ft-sam.py:312] Train: [8][6/20]	                     loss 0.22849303483963013 (0.2371654689311981	                     Acc@1 0.9765625 (0.98125
2024-11-17:17:06:33 [INFO    ] [ft-sam.py:312] Train: [8][7/20]	                     loss 0.2716192603111267 (0.24290776749451956	                     Acc@1 0.9453125 (0.9752604166666666
2024-11-17:17:06:35 [INFO    ] [ft-sam.py:312] Train: [8][8/20]	                     loss 0.23081544041633606 (0.2411802921976362	                     Acc@1 0.9921875 (0.9776785714285714
2024-11-17:17:06:37 [INFO    ] [ft-sam.py:312] Train: [8][9/20]	                     loss 0.26640477776527405 (0.24433335289359093	                     Acc@1 0.96875 (0.9765625
2024-11-17:17:06:39 [INFO    ] [ft-sam.py:312] Train: [8][10/20]	                     loss 0.24282026290893555 (0.24416523178418478	                     Acc@1 0.9765625 (0.9765625
2024-11-17:17:06:41 [INFO    ] [ft-sam.py:312] Train: [8][11/20]	                     loss 0.23254165053367615 (0.24300287365913392	                     Acc@1 0.9609375 (0.975
2024-11-17:17:06:43 [INFO    ] [ft-sam.py:312] Train: [8][12/20]	                     loss 0.23107482492923737 (0.2419185055927797	                     Acc@1 0.953125 (0.9730113636363636
2024-11-17:17:06:45 [INFO    ] [ft-sam.py:312] Train: [8][13/20]	                     loss 0.2575545907020569 (0.24322151268521944	                     Acc@1 0.96875 (0.97265625
2024-11-17:17:06:47 [INFO    ] [ft-sam.py:312] Train: [8][14/20]	                     loss 0.27112507820129395 (0.24536794080184057	                     Acc@1 0.9453125 (0.9705528846153846
2024-11-17:17:06:49 [INFO    ] [ft-sam.py:312] Train: [8][15/20]	                     loss 0.2256869673728943 (0.24396215698548726	                     Acc@1 0.9453125 (0.96875
2024-11-17:17:06:51 [INFO    ] [ft-sam.py:312] Train: [8][16/20]	                     loss 0.22464874386787415 (0.24267459611097972	                     Acc@1 0.96875 (0.96875
2024-11-17:17:06:53 [INFO    ] [ft-sam.py:312] Train: [8][17/20]	                     loss 0.2392638623714447 (0.24246142525225878	                     Acc@1 0.953125 (0.9677734375
2024-11-17:17:06:55 [INFO    ] [ft-sam.py:312] Train: [8][18/20]	                     loss 0.2545156478881836 (0.24317049717201905	                     Acc@1 0.953125 (0.9669117647058824
2024-11-17:17:06:57 [INFO    ] [ft-sam.py:312] Train: [8][19/20]	                     loss 0.2705038785934448 (0.24468901836209828	                     Acc@1 0.9609375 (0.9665798611111112
2024-11-17:17:06:59 [INFO    ] [ft-sam.py:312] Train: [8][20/20]	                     loss 0.24676726758480072 (0.24479839990013524	                     Acc@1 0.9609375 (0.9662828947368421
2024-11-17:17:07:00 [INFO    ] [ft-sam.py:312] Train: [8][21/20]	                     loss 0.22626681625843048 (0.24429434082508086	                     Acc@1 0.9705882352941176 (0.9664
2024-11-17:17:08:10 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.12047090150520835,
 'clean_test_loss_avg_over_batch': 0.2755127975457831,
 'test_acc': 0.9313,
 'test_asr': 0.955,
 'test_ra': 0.044,
 'train_acc': 0.9664,
 'train_epoch_loss_avg_over_batch': 0.24429434082508086}
2024-11-17:17:08:12 [INFO    ] [ft-sam.py:312] Train: [9][2/20]	                     loss 0.22622403502464294 (0.22622403502464294	                     Acc@1 0.96875 (0.96875
2024-11-17:17:08:14 [INFO    ] [ft-sam.py:312] Train: [9][3/20]	                     loss 0.2062516212463379 (0.21623782813549042	                     Acc@1 0.9765625 (0.97265625
2024-11-17:17:08:16 [INFO    ] [ft-sam.py:312] Train: [9][4/20]	                     loss 0.24014809727668762 (0.2242079178492228	                     Acc@1 0.9609375 (0.96875
2024-11-17:17:08:18 [INFO    ] [ft-sam.py:312] Train: [9][5/20]	                     loss 0.2514774799346924 (0.2310253083705902	                     Acc@1 0.984375 (0.97265625
2024-11-17:17:08:20 [INFO    ] [ft-sam.py:312] Train: [9][6/20]	                     loss 0.24262167513370514 (0.2333445817232132	                     Acc@1 0.96875 (0.971875
2024-11-17:17:08:22 [INFO    ] [ft-sam.py:312] Train: [9][7/20]	                     loss 0.2664870321750641 (0.23886832346518835	                     Acc@1 0.9765625 (0.97265625
2024-11-17:17:08:24 [INFO    ] [ft-sam.py:312] Train: [9][8/20]	                     loss 0.2658163011074066 (0.2427180345569338	                     Acc@1 0.9453125 (0.96875
2024-11-17:17:08:26 [INFO    ] [ft-sam.py:312] Train: [9][9/20]	                     loss 0.22325338423252106 (0.24028495326638222	                     Acc@1 0.9765625 (0.9697265625
2024-11-17:17:08:28 [INFO    ] [ft-sam.py:312] Train: [9][10/20]	                     loss 0.25446590781211853 (0.24186061488257515	                     Acc@1 0.9921875 (0.9722222222222222
2024-11-17:17:08:30 [INFO    ] [ft-sam.py:312] Train: [9][11/20]	                     loss 0.24183273315429688 (0.24185782670974731	                     Acc@1 0.953125 (0.9703125
2024-11-17:17:08:32 [INFO    ] [ft-sam.py:312] Train: [9][12/20]	                     loss 0.23651626706123352 (0.24137223037806424	                     Acc@1 0.9765625 (0.9708806818181818
2024-11-17:17:08:34 [INFO    ] [ft-sam.py:312] Train: [9][13/20]	                     loss 0.21920594573020935 (0.239525039990743	                     Acc@1 0.9765625 (0.9713541666666666
2024-11-17:17:08:35 [INFO    ] [ft-sam.py:312] Train: [9][14/20]	                     loss 0.23106911778450012 (0.23887458443641663	                     Acc@1 0.96875 (0.9711538461538461
2024-11-17:17:08:37 [INFO    ] [ft-sam.py:312] Train: [9][15/20]	                     loss 0.19650602340698242 (0.2358482586485999	                     Acc@1 0.984375 (0.9720982142857143
2024-11-17:17:08:39 [INFO    ] [ft-sam.py:312] Train: [9][16/20]	                     loss 0.22580483555793762 (0.2351786971092224	                     Acc@1 0.984375 (0.9729166666666667
2024-11-17:17:08:41 [INFO    ] [ft-sam.py:312] Train: [9][17/20]	                     loss 0.1798069179058075 (0.23171796090900898	                     Acc@1 0.9921875 (0.97412109375
2024-11-17:17:08:43 [INFO    ] [ft-sam.py:312] Train: [9][18/20]	                     loss 0.20266097784042358 (0.2300087266108569	                     Acc@1 1.0 (0.9756433823529411
2024-11-17:17:08:45 [INFO    ] [ft-sam.py:312] Train: [9][19/20]	                     loss 0.20676594972610474 (0.22871746122837067	                     Acc@1 0.984375 (0.9761284722222222
2024-11-17:17:08:47 [INFO    ] [ft-sam.py:312] Train: [9][20/20]	                     loss 0.21105018258094788 (0.2277876044574537	                     Acc@1 0.9765625 (0.9761513157894737
2024-11-17:17:08:48 [INFO    ] [ft-sam.py:312] Train: [9][21/20]	                     loss 0.2227441519498825 (0.22765042254924775	                     Acc@1 0.9852941176470589 (0.9764
2024-11-17:17:09:58 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.16135839850340092,
 'clean_test_loss_avg_over_batch': 0.2642564471763901,
 'test_acc': 0.9361,
 'test_asr': 0.9395555555555556,
 'test_ra': 0.058444444444444445,
 'train_acc': 0.9764,
 'train_epoch_loss_avg_over_batch': 0.22765042254924775}
2024-11-17:17:10:00 [INFO    ] [ft-sam.py:312] Train: [10][2/20]	                     loss 0.1856941431760788 (0.1856941431760788	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:10:02 [INFO    ] [ft-sam.py:312] Train: [10][3/20]	                     loss 0.22440016269683838 (0.2050471529364586	                     Acc@1 0.9765625 (0.984375
2024-11-17:17:10:04 [INFO    ] [ft-sam.py:312] Train: [10][4/20]	                     loss 0.23617123067378998 (0.21542184551556906	                     Acc@1 0.9453125 (0.9713541666666666
2024-11-17:17:10:06 [INFO    ] [ft-sam.py:312] Train: [10][5/20]	                     loss 0.2093050628900528 (0.21389264985919	                     Acc@1 0.9921875 (0.9765625
2024-11-17:17:10:08 [INFO    ] [ft-sam.py:312] Train: [10][6/20]	                     loss 0.19995667040348053 (0.21110545396804808	                     Acc@1 0.9921875 (0.9796875
2024-11-17:17:10:10 [INFO    ] [ft-sam.py:312] Train: [10][7/20]	                     loss 0.20114848017692566 (0.20944595833619437	                     Acc@1 0.984375 (0.98046875
2024-11-17:17:10:12 [INFO    ] [ft-sam.py:312] Train: [10][8/20]	                     loss 0.2091846466064453 (0.20940862808908736	                     Acc@1 0.9765625 (0.9799107142857143
2024-11-17:17:10:14 [INFO    ] [ft-sam.py:312] Train: [10][9/20]	                     loss 0.20204707980155945 (0.20848843455314636	                     Acc@1 0.96875 (0.978515625
2024-11-17:17:10:16 [INFO    ] [ft-sam.py:312] Train: [10][10/20]	                     loss 0.19802680611610413 (0.207326031393475	                     Acc@1 0.96875 (0.9774305555555556
2024-11-17:17:10:18 [INFO    ] [ft-sam.py:312] Train: [10][11/20]	                     loss 0.2246304452419281 (0.20905647277832032	                     Acc@1 0.9765625 (0.97734375
2024-11-17:17:10:20 [INFO    ] [ft-sam.py:312] Train: [10][12/20]	                     loss 0.2215387225151062 (0.21019122275439175	                     Acc@1 0.9765625 (0.9772727272727273
2024-11-17:17:10:22 [INFO    ] [ft-sam.py:312] Train: [10][13/20]	                     loss 0.22018977999687195 (0.21102443585793176	                     Acc@1 0.96875 (0.9765625
2024-11-17:17:10:24 [INFO    ] [ft-sam.py:312] Train: [10][14/20]	                     loss 0.19933408498764038 (0.2101251780986786	                     Acc@1 1.0 (0.9783653846153846
2024-11-17:17:10:26 [INFO    ] [ft-sam.py:312] Train: [10][15/20]	                     loss 0.21005135774612427 (0.2101199052163533	                     Acc@1 0.96875 (0.9776785714285714
2024-11-17:17:10:28 [INFO    ] [ft-sam.py:312] Train: [10][16/20]	                     loss 0.20910626649856567 (0.2100523293018341	                     Acc@1 0.96875 (0.9770833333333333
2024-11-17:17:10:30 [INFO    ] [ft-sam.py:312] Train: [10][17/20]	                     loss 0.1897706687450409 (0.20878472551703453	                     Acc@1 0.984375 (0.9775390625
2024-11-17:17:10:32 [INFO    ] [ft-sam.py:312] Train: [10][18/20]	                     loss 0.1882459819316864 (0.2075765641296611	                     Acc@1 1.0 (0.9788602941176471
2024-11-17:17:10:34 [INFO    ] [ft-sam.py:312] Train: [10][19/20]	                     loss 0.19622553884983063 (0.20694595161411497	                     Acc@1 0.984375 (0.9791666666666666
2024-11-17:17:10:36 [INFO    ] [ft-sam.py:312] Train: [10][20/20]	                     loss 0.18436142802238464 (0.2057572924777081	                     Acc@1 0.9765625 (0.9790296052631579
2024-11-17:17:10:37 [INFO    ] [ft-sam.py:312] Train: [10][21/20]	                     loss 0.20371343195438385 (0.20570169947147368	                     Acc@1 0.9705882352941176 (0.9788
2024-11-17:17:11:46 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.16152229606055876,
 'clean_test_loss_avg_over_batch': 0.26278099398824234,
 'test_acc': 0.9409,
 'test_asr': 0.9396666666666667,
 'test_ra': 0.058222222222222224,
 'train_acc': 0.9788,
 'train_epoch_loss_avg_over_batch': 0.20570169947147368}
2024-11-17:17:11:49 [INFO    ] [ft-sam.py:312] Train: [11][2/20]	                     loss 0.18641212582588196 (0.18641212582588196	                     Acc@1 0.9765625 (0.9765625
2024-11-17:17:11:51 [INFO    ] [ft-sam.py:312] Train: [11][3/20]	                     loss 0.18053606152534485 (0.1834740936756134	                     Acc@1 0.9765625 (0.9765625
2024-11-17:17:11:52 [INFO    ] [ft-sam.py:312] Train: [11][4/20]	                     loss 0.184190034866333 (0.18371274073918661	                     Acc@1 0.984375 (0.9791666666666666
2024-11-17:17:11:54 [INFO    ] [ft-sam.py:312] Train: [11][5/20]	                     loss 0.1689395308494568 (0.18001943826675415	                     Acc@1 1.0 (0.984375
2024-11-17:17:11:56 [INFO    ] [ft-sam.py:312] Train: [11][6/20]	                     loss 0.1697513461112976 (0.17796581983566284	                     Acc@1 0.9765625 (0.9828125
2024-11-17:17:11:58 [INFO    ] [ft-sam.py:312] Train: [11][7/20]	                     loss 0.19346284866333008 (0.1805486579736074	                     Acc@1 0.9921875 (0.984375
2024-11-17:17:12:00 [INFO    ] [ft-sam.py:312] Train: [11][8/20]	                     loss 0.22075155377388 (0.18629192880221776	                     Acc@1 0.96875 (0.9821428571428571
2024-11-17:17:12:02 [INFO    ] [ft-sam.py:312] Train: [11][9/20]	                     loss 0.2068735957145691 (0.18886463716626167	                     Acc@1 0.984375 (0.982421875
2024-11-17:17:12:04 [INFO    ] [ft-sam.py:312] Train: [11][10/20]	                     loss 0.17082755267620087 (0.18686051666736603	                     Acc@1 0.984375 (0.9826388888888888
2024-11-17:17:12:06 [INFO    ] [ft-sam.py:312] Train: [11][11/20]	                     loss 0.184980571269989 (0.18667252212762833	                     Acc@1 0.984375 (0.9828125
2024-11-17:17:12:08 [INFO    ] [ft-sam.py:312] Train: [11][12/20]	                     loss 0.18444164097309113 (0.18646971474994312	                     Acc@1 0.9921875 (0.9836647727272727
2024-11-17:17:12:10 [INFO    ] [ft-sam.py:312] Train: [11][13/20]	                     loss 0.1649666577577591 (0.18467779333392778	                     Acc@1 0.9921875 (0.984375
2024-11-17:17:12:12 [INFO    ] [ft-sam.py:312] Train: [11][14/20]	                     loss 0.20464786887168884 (0.18621395299067864	                     Acc@1 0.9765625 (0.9837740384615384
2024-11-17:17:12:14 [INFO    ] [ft-sam.py:312] Train: [11][15/20]	                     loss 0.16734692454338074 (0.18486630810158594	                     Acc@1 0.984375 (0.9838169642857143
2024-11-17:17:12:16 [INFO    ] [ft-sam.py:312] Train: [11][16/20]	                     loss 0.1846325546503067 (0.1848507245381673	                     Acc@1 0.984375 (0.9838541666666667
2024-11-17:17:12:18 [INFO    ] [ft-sam.py:312] Train: [11][17/20]	                     loss 0.18068180978298187 (0.18459016736596823	                     Acc@1 0.984375 (0.98388671875
2024-11-17:17:12:20 [INFO    ] [ft-sam.py:312] Train: [11][18/20]	                     loss 0.1511942744255066 (0.18262570307535284	                     Acc@1 1.0 (0.9848345588235294
2024-11-17:17:12:22 [INFO    ] [ft-sam.py:312] Train: [11][19/20]	                     loss 0.18421047925949097 (0.18271374619669384	                     Acc@1 0.96875 (0.9839409722222222
2024-11-17:17:12:24 [INFO    ] [ft-sam.py:312] Train: [11][20/20]	                     loss 0.15352413058280945 (0.18117745063806834	                     Acc@1 1.0 (0.9847861842105263
2024-11-17:17:12:25 [INFO    ] [ft-sam.py:312] Train: [11][21/20]	                     loss 0.19020457565784454 (0.18142298843860627	                     Acc@1 0.9852941176470589 (0.9848
2024-11-17:17:13:35 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.15911450908637384,
 'clean_test_loss_avg_over_batch': 0.2566829406762425,
 'test_acc': 0.9446,
 'test_asr': 0.9405555555555556,
 'test_ra': 0.05733333333333333,
 'train_acc': 0.9848,
 'train_epoch_loss_avg_over_batch': 0.18142298843860627}
2024-11-17:17:13:37 [INFO    ] [ft-sam.py:312] Train: [12][2/20]	                     loss 0.17292656004428864 (0.17292656004428864	                     Acc@1 0.984375 (0.984375
2024-11-17:17:13:39 [INFO    ] [ft-sam.py:312] Train: [12][3/20]	                     loss 0.1716451644897461 (0.17228586226701736	                     Acc@1 0.984375 (0.984375
2024-11-17:17:13:41 [INFO    ] [ft-sam.py:312] Train: [12][4/20]	                     loss 0.16665104031562805 (0.17040758828322092	                     Acc@1 0.9921875 (0.9869791666666666
2024-11-17:17:13:43 [INFO    ] [ft-sam.py:312] Train: [12][5/20]	                     loss 0.17911797761917114 (0.17258518561720848	                     Acc@1 0.9921875 (0.98828125
2024-11-17:17:13:45 [INFO    ] [ft-sam.py:312] Train: [12][6/20]	                     loss 0.17500072717666626 (0.17306829392910003	                     Acc@1 1.0 (0.990625
2024-11-17:17:13:47 [INFO    ] [ft-sam.py:312] Train: [12][7/20]	                     loss 0.16348978877067566 (0.17147187640269598	                     Acc@1 1.0 (0.9921875
2024-11-17:17:13:49 [INFO    ] [ft-sam.py:312] Train: [12][8/20]	                     loss 0.15912681818008423 (0.16970829665660858	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:13:51 [INFO    ] [ft-sam.py:312] Train: [12][9/20]	                     loss 0.16549856960773468 (0.16918208077549934	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:13:53 [INFO    ] [ft-sam.py:312] Train: [12][10/20]	                     loss 0.18312248587608337 (0.17073101467556423	                     Acc@1 0.984375 (0.9913194444444444
2024-11-17:17:13:55 [INFO    ] [ft-sam.py:312] Train: [12][11/20]	                     loss 0.17090679705142975 (0.17074859291315078	                     Acc@1 0.984375 (0.990625
2024-11-17:17:13:57 [INFO    ] [ft-sam.py:312] Train: [12][12/20]	                     loss 0.15909051895141602 (0.16968876800753854	                     Acc@1 0.9921875 (0.9907670454545454
2024-11-17:17:13:59 [INFO    ] [ft-sam.py:312] Train: [12][13/20]	                     loss 0.17713023722171783 (0.17030889044205347	                     Acc@1 0.9921875 (0.9908854166666666
2024-11-17:17:14:01 [INFO    ] [ft-sam.py:312] Train: [12][14/20]	                     loss 0.1456880122423172 (0.16841497673438147	                     Acc@1 0.9921875 (0.9909855769230769
2024-11-17:17:14:02 [INFO    ] [ft-sam.py:312] Train: [12][15/20]	                     loss 0.17046773433685303 (0.16856160227741515	                     Acc@1 0.984375 (0.9905133928571429
2024-11-17:17:14:04 [INFO    ] [ft-sam.py:312] Train: [12][16/20]	                     loss 0.15493880212306976 (0.1676534156004588	                     Acc@1 1.0 (0.9911458333333333
2024-11-17:17:14:06 [INFO    ] [ft-sam.py:312] Train: [12][17/20]	                     loss 0.17535936832427979 (0.1681350376456976	                     Acc@1 0.984375 (0.99072265625
2024-11-17:17:14:08 [INFO    ] [ft-sam.py:312] Train: [12][18/20]	                     loss 0.17464759945869446 (0.16851812951705036	                     Acc@1 0.9921875 (0.9908088235294118
2024-11-17:17:14:10 [INFO    ] [ft-sam.py:312] Train: [12][19/20]	                     loss 0.1690058410167694 (0.16854522460036808	                     Acc@1 0.9765625 (0.9900173611111112
2024-11-17:17:14:12 [INFO    ] [ft-sam.py:312] Train: [12][20/20]	                     loss 0.17095637321472168 (0.16867212715901828	                     Acc@1 0.96875 (0.9888980263157895
2024-11-17:17:14:13 [INFO    ] [ft-sam.py:312] Train: [12][21/20]	                     loss 0.1492265909910202 (0.16814320857524873	                     Acc@1 0.9705882352941176 (0.9884
2024-11-17:17:15:23 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.1607388029216041,
 'clean_test_loss_avg_over_batch': 0.2790112910391409,
 'test_acc': 0.9447,
 'test_asr': 0.9403333333333334,
 'test_ra': 0.057444444444444444,
 'train_acc': 0.9884,
 'train_epoch_loss_avg_over_batch': 0.16814320857524873}
2024-11-17:17:15:25 [INFO    ] [ft-sam.py:312] Train: [13][2/20]	                     loss 0.15388020873069763 (0.15388020873069763	                     Acc@1 1.0 (1.0
2024-11-17:17:15:27 [INFO    ] [ft-sam.py:312] Train: [13][3/20]	                     loss 0.18134167790412903 (0.16761094331741333	                     Acc@1 0.984375 (0.9921875
2024-11-17:17:15:29 [INFO    ] [ft-sam.py:312] Train: [13][4/20]	                     loss 0.14605791866779327 (0.16042660176753998	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:15:31 [INFO    ] [ft-sam.py:312] Train: [13][5/20]	                     loss 0.16146613657474518 (0.16068648546934128	                     Acc@1 1.0 (0.994140625
2024-11-17:17:15:33 [INFO    ] [ft-sam.py:312] Train: [13][6/20]	                     loss 0.1442355215549469 (0.1573962926864624	                     Acc@1 0.9921875 (0.99375
2024-11-17:17:15:35 [INFO    ] [ft-sam.py:312] Train: [13][7/20]	                     loss 0.15311527252197266 (0.1566827893257141	                     Acc@1 1.0 (0.9947916666666666
2024-11-17:17:15:37 [INFO    ] [ft-sam.py:312] Train: [13][8/20]	                     loss 0.1503833383321762 (0.1557828677552087	                     Acc@1 0.984375 (0.9933035714285714
2024-11-17:17:15:39 [INFO    ] [ft-sam.py:312] Train: [13][9/20]	                     loss 0.14855316281318665 (0.15487915463745594	                     Acc@1 0.9921875 (0.9931640625
2024-11-17:17:15:41 [INFO    ] [ft-sam.py:312] Train: [13][10/20]	                     loss 0.19021598994731903 (0.15880546967188516	                     Acc@1 0.96875 (0.9904513888888888
2024-11-17:17:15:43 [INFO    ] [ft-sam.py:312] Train: [13][11/20]	                     loss 0.16097122430801392 (0.15902204513549806	                     Acc@1 0.9921875 (0.990625
2024-11-17:17:15:45 [INFO    ] [ft-sam.py:312] Train: [13][12/20]	                     loss 0.1513361632823944 (0.1583233286033977	                     Acc@1 1.0 (0.9914772727272727
2024-11-17:17:15:47 [INFO    ] [ft-sam.py:312] Train: [13][13/20]	                     loss 0.14939850568771362 (0.15757959336042404	                     Acc@1 1.0 (0.9921875
2024-11-17:17:15:49 [INFO    ] [ft-sam.py:312] Train: [13][14/20]	                     loss 0.14917868375778198 (0.1569333695448362	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:15:51 [INFO    ] [ft-sam.py:312] Train: [13][15/20]	                     loss 0.18894371390342712 (0.15921982271330698	                     Acc@1 0.953125 (0.9893973214285714
2024-11-17:17:15:53 [INFO    ] [ft-sam.py:312] Train: [13][16/20]	                     loss 0.13722147047519684 (0.15775326589743296	                     Acc@1 1.0 (0.9901041666666667
2024-11-17:17:15:55 [INFO    ] [ft-sam.py:312] Train: [13][17/20]	                     loss 0.13759422302246094 (0.1564933257177472	                     Acc@1 1.0 (0.99072265625
2024-11-17:17:15:57 [INFO    ] [ft-sam.py:312] Train: [13][18/20]	                     loss 0.1508176624774933 (0.15615946317420287	                     Acc@1 0.9921875 (0.9908088235294118
2024-11-17:17:15:59 [INFO    ] [ft-sam.py:312] Train: [13][19/20]	                     loss 0.14210203289985657 (0.15537849482562807	                     Acc@1 0.9921875 (0.9908854166666666
2024-11-17:17:16:01 [INFO    ] [ft-sam.py:312] Train: [13][20/20]	                     loss 0.14663530886173248 (0.15491832714331777	                     Acc@1 0.9921875 (0.990953947368421
2024-11-17:17:16:02 [INFO    ] [ft-sam.py:312] Train: [13][21/20]	                     loss 0.1693660318851471 (0.15531130471229554	                     Acc@1 0.9852941176470589 (0.9908
2024-11-17:17:17:11 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.17242784987033252,
 'clean_test_loss_avg_over_batch': 0.2616162196367602,
 'test_acc': 0.9475,
 'test_asr': 0.9367777777777778,
 'test_ra': 0.06077777777777778,
 'train_acc': 0.9908,
 'train_epoch_loss_avg_over_batch': 0.15531130471229554}
2024-11-17:17:17:14 [INFO    ] [ft-sam.py:312] Train: [14][2/20]	                     loss 0.12871262431144714 (0.12871262431144714	                     Acc@1 1.0 (1.0
2024-11-17:17:17:16 [INFO    ] [ft-sam.py:312] Train: [14][3/20]	                     loss 0.16958725452423096 (0.14914993941783905	                     Acc@1 0.9765625 (0.98828125
2024-11-17:17:17:18 [INFO    ] [ft-sam.py:312] Train: [14][4/20]	                     loss 0.1608782410621643 (0.1530593732992808	                     Acc@1 0.9921875 (0.9895833333333334
2024-11-17:17:17:20 [INFO    ] [ft-sam.py:312] Train: [14][5/20]	                     loss 0.15938931703567505 (0.15464185923337936	                     Acc@1 0.984375 (0.98828125
2024-11-17:17:17:22 [INFO    ] [ft-sam.py:312] Train: [14][6/20]	                     loss 0.13794349133968353 (0.1513021856546402	                     Acc@1 0.9921875 (0.9890625
2024-11-17:17:17:23 [INFO    ] [ft-sam.py:312] Train: [14][7/20]	                     loss 0.14108172059059143 (0.14959877481063208	                     Acc@1 0.9921875 (0.9895833333333334
2024-11-17:17:17:25 [INFO    ] [ft-sam.py:312] Train: [14][8/20]	                     loss 0.122600257396698 (0.14574184375149862	                     Acc@1 1.0 (0.9910714285714286
2024-11-17:17:17:27 [INFO    ] [ft-sam.py:312] Train: [14][9/20]	                     loss 0.16686072945594788 (0.1483817044645548	                     Acc@1 1.0 (0.9921875
2024-11-17:17:17:29 [INFO    ] [ft-sam.py:312] Train: [14][10/20]	                     loss 0.161849707365036 (0.14987814923127493	                     Acc@1 0.984375 (0.9913194444444444
2024-11-17:17:17:31 [INFO    ] [ft-sam.py:312] Train: [14][11/20]	                     loss 0.15341317653656006 (0.15023165196180344	                     Acc@1 0.984375 (0.990625
2024-11-17:17:17:33 [INFO    ] [ft-sam.py:312] Train: [14][12/20]	                     loss 0.1317906379699707 (0.1485551961443641	                     Acc@1 0.984375 (0.9900568181818182
2024-11-17:17:17:35 [INFO    ] [ft-sam.py:312] Train: [14][13/20]	                     loss 0.1485888510942459 (0.1485580007235209	                     Acc@1 1.0 (0.9908854166666666
2024-11-17:17:17:37 [INFO    ] [ft-sam.py:312] Train: [14][14/20]	                     loss 0.1285407841205597 (0.14701821483098543	                     Acc@1 1.0 (0.9915865384615384
2024-11-17:17:17:39 [INFO    ] [ft-sam.py:312] Train: [14][15/20]	                     loss 0.1906583309173584 (0.15013536598001206	                     Acc@1 0.9765625 (0.9905133928571429
2024-11-17:17:17:41 [INFO    ] [ft-sam.py:312] Train: [14][16/20]	                     loss 0.1447027623653412 (0.1497731924057007	                     Acc@1 0.984375 (0.9901041666666667
2024-11-17:17:17:43 [INFO    ] [ft-sam.py:312] Train: [14][17/20]	                     loss 0.13582319021224976 (0.14890131726861	                     Acc@1 1.0 (0.99072265625
2024-11-17:17:17:45 [INFO    ] [ft-sam.py:312] Train: [14][18/20]	                     loss 0.16397815942764282 (0.1497881903367884	                     Acc@1 0.984375 (0.9903492647058824
2024-11-17:17:17:47 [INFO    ] [ft-sam.py:312] Train: [14][19/20]	                     loss 0.20811788737773895 (0.15302872906128565	                     Acc@1 0.9765625 (0.9895833333333334
2024-11-17:17:17:49 [INFO    ] [ft-sam.py:312] Train: [14][20/20]	                     loss 0.1573420912027359 (0.15325574812136197	                     Acc@1 0.9921875 (0.9897203947368421
2024-11-17:17:17:50 [INFO    ] [ft-sam.py:312] Train: [14][21/20]	                     loss 0.1293042153120041 (0.15260426642894745	                     Acc@1 1.0 (0.99
2024-11-17:17:19:00 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.17617439604561094,
 'clean_test_loss_avg_over_batch': 0.2669322307728514,
 'test_acc': 0.948,
 'test_asr': 0.9362222222222222,
 'test_ra': 0.06122222222222222,
 'train_acc': 0.99,
 'train_epoch_loss_avg_over_batch': 0.15260426642894745}
2024-11-17:17:19:02 [INFO    ] [ft-sam.py:312] Train: [15][2/20]	                     loss 0.14687156677246094 (0.14687156677246094	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:19:04 [INFO    ] [ft-sam.py:312] Train: [15][3/20]	                     loss 0.14844223856925964 (0.1476569026708603	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:19:06 [INFO    ] [ft-sam.py:312] Train: [15][4/20]	                     loss 0.15485113859176636 (0.15005498131116232	                     Acc@1 0.984375 (0.9895833333333334
2024-11-17:17:19:08 [INFO    ] [ft-sam.py:312] Train: [15][5/20]	                     loss 0.14012886583805084 (0.14757345244288445	                     Acc@1 1.0 (0.9921875
2024-11-17:17:19:10 [INFO    ] [ft-sam.py:312] Train: [15][6/20]	                     loss 0.14681866765022278 (0.1474224954843521	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:19:12 [INFO    ] [ft-sam.py:312] Train: [15][7/20]	                     loss 0.15545786917209625 (0.14876172443230948	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:19:14 [INFO    ] [ft-sam.py:312] Train: [15][8/20]	                     loss 0.14732474088668823 (0.1485564410686493	                     Acc@1 0.984375 (0.9910714285714286
2024-11-17:17:19:16 [INFO    ] [ft-sam.py:312] Train: [15][9/20]	                     loss 0.14768439531326294 (0.148447435349226	                     Acc@1 0.9921875 (0.9912109375
2024-11-17:17:19:18 [INFO    ] [ft-sam.py:312] Train: [15][10/20]	                     loss 0.13322336971759796 (0.14675587250126731	                     Acc@1 1.0 (0.9921875
2024-11-17:17:19:20 [INFO    ] [ft-sam.py:312] Train: [15][11/20]	                     loss 0.18455466628074646 (0.15053575187921525	                     Acc@1 0.96875 (0.98984375
2024-11-17:17:19:22 [INFO    ] [ft-sam.py:312] Train: [15][12/20]	                     loss 0.13753587007522583 (0.14935394444248892	                     Acc@1 1.0 (0.9907670454545454
2024-11-17:17:19:24 [INFO    ] [ft-sam.py:312] Train: [15][13/20]	                     loss 0.16537655889987946 (0.15068916231393814	                     Acc@1 0.9921875 (0.9908854166666666
2024-11-17:17:19:26 [INFO    ] [ft-sam.py:312] Train: [15][14/20]	                     loss 0.14975351095199585 (0.15061718913225028	                     Acc@1 1.0 (0.9915865384615384
2024-11-17:17:19:28 [INFO    ] [ft-sam.py:312] Train: [15][15/20]	                     loss 0.14700864255428314 (0.15035943580525263	                     Acc@1 0.9921875 (0.9916294642857143
2024-11-17:17:19:30 [INFO    ] [ft-sam.py:312] Train: [15][16/20]	                     loss 0.15386396646499634 (0.15059307118256887	                     Acc@1 0.984375 (0.9911458333333333
2024-11-17:17:19:32 [INFO    ] [ft-sam.py:312] Train: [15][17/20]	                     loss 0.15576103329658508 (0.15091606881469488	                     Acc@1 0.984375 (0.99072265625
2024-11-17:17:19:34 [INFO    ] [ft-sam.py:312] Train: [15][18/20]	                     loss 0.14718857407569885 (0.15069680441828334	                     Acc@1 0.984375 (0.9903492647058824
2024-11-17:17:19:35 [INFO    ] [ft-sam.py:312] Train: [15][19/20]	                     loss 0.1583377718925476 (0.15112130261129803	                     Acc@1 0.9921875 (0.9904513888888888
2024-11-17:17:19:37 [INFO    ] [ft-sam.py:312] Train: [15][20/20]	                     loss 0.13794615864753723 (0.1504278739816264	                     Acc@1 1.0 (0.990953947368421
2024-11-17:17:19:39 [INFO    ] [ft-sam.py:312] Train: [15][21/20]	                     loss 0.13634812831878662 (0.15004490489959718	                     Acc@1 0.9852941176470589 (0.9908
2024-11-17:17:20:48 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.17318139062591,
 'clean_test_loss_avg_over_batch': 0.2651177952183953,
 'test_acc': 0.9482,
 'test_asr': 0.9365555555555556,
 'test_ra': 0.06088888888888889,
 'train_acc': 0.9908,
 'train_epoch_loss_avg_over_batch': 0.15004490489959718}
2024-11-17:17:20:50 [INFO    ] [ft-sam.py:312] Train: [16][2/20]	                     loss 0.16532349586486816 (0.16532349586486816	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:20:52 [INFO    ] [ft-sam.py:312] Train: [16][3/20]	                     loss 0.14583873748779297 (0.15558111667633057	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:20:54 [INFO    ] [ft-sam.py:312] Train: [16][4/20]	                     loss 0.1432601511478424 (0.1514741281668345	                     Acc@1 1.0 (0.9947916666666666
2024-11-17:17:20:56 [INFO    ] [ft-sam.py:312] Train: [16][5/20]	                     loss 0.1519535481929779 (0.15159398317337036	                     Acc@1 0.9921875 (0.994140625
2024-11-17:17:20:58 [INFO    ] [ft-sam.py:312] Train: [16][6/20]	                     loss 0.15447045862674713 (0.15216927826404572	                     Acc@1 0.9921875 (0.99375
2024-11-17:17:21:00 [INFO    ] [ft-sam.py:312] Train: [16][7/20]	                     loss 0.12901130318641663 (0.14830961575110754	                     Acc@1 1.0 (0.9947916666666666
2024-11-17:17:21:02 [INFO    ] [ft-sam.py:312] Train: [16][8/20]	                     loss 0.1511937826871872 (0.1487216395991189	                     Acc@1 0.9921875 (0.9944196428571429
2024-11-17:17:21:04 [INFO    ] [ft-sam.py:312] Train: [16][9/20]	                     loss 0.15535272657871246 (0.1495505254715681	                     Acc@1 0.9921875 (0.994140625
2024-11-17:17:21:06 [INFO    ] [ft-sam.py:312] Train: [16][10/20]	                     loss 0.14586713910102844 (0.14914126031928593	                     Acc@1 0.9921875 (0.9939236111111112
2024-11-17:17:21:08 [INFO    ] [ft-sam.py:312] Train: [16][11/20]	                     loss 0.17713028192520142 (0.15194016247987746	                     Acc@1 0.9765625 (0.9921875
2024-11-17:17:21:10 [INFO    ] [ft-sam.py:312] Train: [16][12/20]	                     loss 0.15806470811367035 (0.15249693935567682	                     Acc@1 0.984375 (0.9914772727272727
2024-11-17:17:21:12 [INFO    ] [ft-sam.py:312] Train: [16][13/20]	                     loss 0.16386273503303528 (0.1534440889954567	                     Acc@1 0.984375 (0.9908854166666666
2024-11-17:17:21:14 [INFO    ] [ft-sam.py:312] Train: [16][14/20]	                     loss 0.14325404167175293 (0.15266023920132563	                     Acc@1 1.0 (0.9915865384615384
2024-11-17:17:21:16 [INFO    ] [ft-sam.py:312] Train: [16][15/20]	                     loss 0.16478268802165985 (0.15352612840277807	                     Acc@1 0.9765625 (0.9905133928571429
2024-11-17:17:21:18 [INFO    ] [ft-sam.py:312] Train: [16][16/20]	                     loss 0.15784518420696259 (0.15381406545639037	                     Acc@1 0.984375 (0.9901041666666667
2024-11-17:17:21:20 [INFO    ] [ft-sam.py:312] Train: [16][17/20]	                     loss 0.1574581414461136 (0.15404182020574808	                     Acc@1 0.9921875 (0.990234375
2024-11-17:17:21:22 [INFO    ] [ft-sam.py:312] Train: [16][18/20]	                     loss 0.13716812431812286 (0.15304924985941717	                     Acc@1 0.984375 (0.9898897058823529
2024-11-17:17:21:24 [INFO    ] [ft-sam.py:312] Train: [16][19/20]	                     loss 0.129517063498497 (0.1517419061726994	                     Acc@1 0.9921875 (0.9900173611111112
2024-11-17:17:21:26 [INFO    ] [ft-sam.py:312] Train: [16][20/20]	                     loss 0.14593902230262756 (0.1514364912321693	                     Acc@1 0.984375 (0.9897203947368421
2024-11-17:17:21:27 [INFO    ] [ft-sam.py:312] Train: [16][21/20]	                     loss 0.13029050827026367 (0.15086132049560547	                     Acc@1 1.0 (0.99
2024-11-17:17:22:37 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.17299144987908888,
 'clean_test_loss_avg_over_batch': 0.26694711043110375,
 'test_acc': 0.9479,
 'test_asr': 0.9365555555555556,
 'test_ra': 0.06088888888888889,
 'train_acc': 0.99,
 'train_epoch_loss_avg_over_batch': 0.15086132049560547}
2024-11-17:17:22:39 [INFO    ] [ft-sam.py:312] Train: [17][2/20]	                     loss 0.1457972526550293 (0.1457972526550293	                     Acc@1 0.984375 (0.984375
2024-11-17:17:22:41 [INFO    ] [ft-sam.py:312] Train: [17][3/20]	                     loss 0.16163358092308044 (0.15371541678905487	                     Acc@1 1.0 (0.9921875
2024-11-17:17:22:43 [INFO    ] [ft-sam.py:312] Train: [17][4/20]	                     loss 0.15711086988449097 (0.15484723448753357	                     Acc@1 0.984375 (0.9895833333333334
2024-11-17:17:22:45 [INFO    ] [ft-sam.py:312] Train: [17][5/20]	                     loss 0.16844022274017334 (0.1582454815506935	                     Acc@1 0.9765625 (0.986328125
2024-11-17:17:22:47 [INFO    ] [ft-sam.py:312] Train: [17][6/20]	                     loss 0.13700330257415771 (0.15399704575538636	                     Acc@1 0.9921875 (0.9875
2024-11-17:17:22:49 [INFO    ] [ft-sam.py:312] Train: [17][7/20]	                     loss 0.14953452348709106 (0.1532532920440038	                     Acc@1 0.9921875 (0.98828125
2024-11-17:17:22:51 [INFO    ] [ft-sam.py:312] Train: [17][8/20]	                     loss 0.17138734459877014 (0.155843870980399	                     Acc@1 0.9921875 (0.9888392857142857
2024-11-17:17:22:53 [INFO    ] [ft-sam.py:312] Train: [17][9/20]	                     loss 0.14310622215270996 (0.15425166487693787	                     Acc@1 0.9921875 (0.9892578125
2024-11-17:17:22:55 [INFO    ] [ft-sam.py:312] Train: [17][10/20]	                     loss 0.14949923753738403 (0.15372361739476523	                     Acc@1 0.9921875 (0.9895833333333334
2024-11-17:17:22:57 [INFO    ] [ft-sam.py:312] Train: [17][11/20]	                     loss 0.14561617374420166 (0.15291287302970885	                     Acc@1 0.984375 (0.9890625
2024-11-17:17:22:59 [INFO    ] [ft-sam.py:312] Train: [17][12/20]	                     loss 0.14567649364471436 (0.15225502035834573	                     Acc@1 0.9921875 (0.9893465909090909
2024-11-17:17:23:00 [INFO    ] [ft-sam.py:312] Train: [17][13/20]	                     loss 0.17575840651988983 (0.15421363587180772	                     Acc@1 0.984375 (0.9889322916666666
2024-11-17:17:23:02 [INFO    ] [ft-sam.py:312] Train: [17][14/20]	                     loss 0.15082602202892303 (0.15395305019158584	                     Acc@1 1.0 (0.9897836538461539
2024-11-17:17:23:04 [INFO    ] [ft-sam.py:312] Train: [17][15/20]	                     loss 0.1430143415927887 (0.15317171386310033	                     Acc@1 0.9921875 (0.9899553571428571
2024-11-17:17:23:06 [INFO    ] [ft-sam.py:312] Train: [17][16/20]	                     loss 0.1398346722126007 (0.15228257775306703	                     Acc@1 0.984375 (0.9895833333333334
2024-11-17:17:23:08 [INFO    ] [ft-sam.py:312] Train: [17][17/20]	                     loss 0.13647526502609253 (0.1512946207076311	                     Acc@1 0.9921875 (0.98974609375
2024-11-17:17:23:10 [INFO    ] [ft-sam.py:312] Train: [17][18/20]	                     loss 0.135334312915802 (0.15035577907281764	                     Acc@1 0.984375 (0.9894301470588235
2024-11-17:17:23:12 [INFO    ] [ft-sam.py:312] Train: [17][19/20]	                     loss 0.14413127303123474 (0.15000997318161857	                     Acc@1 0.984375 (0.9891493055555556
2024-11-17:17:23:14 [INFO    ] [ft-sam.py:312] Train: [17][20/20]	                     loss 0.1461273729801178 (0.14980562580259224	                     Acc@1 0.9921875 (0.9893092105263158
2024-11-17:17:23:15 [INFO    ] [ft-sam.py:312] Train: [17][21/20]	                     loss 0.13435089588165283 (0.14938525714874268	                     Acc@1 1.0 (0.9896
2024-11-17:17:24:25 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.17404493553117967,
 'clean_test_loss_avg_over_batch': 0.2654255443735968,
 'test_acc': 0.9484,
 'test_asr': 0.9357777777777778,
 'test_ra': 0.06166666666666667,
 'train_acc': 0.9896,
 'train_epoch_loss_avg_over_batch': 0.14938525714874268}
2024-11-17:17:24:27 [INFO    ] [ft-sam.py:312] Train: [18][2/20]	                     loss 0.1487286388874054 (0.1487286388874054	                     Acc@1 0.984375 (0.984375
2024-11-17:17:24:29 [INFO    ] [ft-sam.py:312] Train: [18][3/20]	                     loss 0.1480514407157898 (0.1483900398015976	                     Acc@1 1.0 (0.9921875
2024-11-17:17:24:31 [INFO    ] [ft-sam.py:312] Train: [18][4/20]	                     loss 0.15534567832946777 (0.15070858597755432	                     Acc@1 0.9765625 (0.9869791666666666
2024-11-17:17:24:33 [INFO    ] [ft-sam.py:312] Train: [18][5/20]	                     loss 0.15222254395484924 (0.15108707547187805	                     Acc@1 0.96875 (0.982421875
2024-11-17:17:24:35 [INFO    ] [ft-sam.py:312] Train: [18][6/20]	                     loss 0.17135804891586304 (0.15514127016067505	                     Acc@1 0.984375 (0.9828125
2024-11-17:17:24:37 [INFO    ] [ft-sam.py:312] Train: [18][7/20]	                     loss 0.13600122928619385 (0.1519512633482615	                     Acc@1 0.9765625 (0.9817708333333334
2024-11-17:17:24:39 [INFO    ] [ft-sam.py:312] Train: [18][8/20]	                     loss 0.16504332423210144 (0.15382155776023865	                     Acc@1 0.9921875 (0.9832589285714286
2024-11-17:17:24:41 [INFO    ] [ft-sam.py:312] Train: [18][9/20]	                     loss 0.13425207138061523 (0.15137537196278572	                     Acc@1 0.9921875 (0.984375
2024-11-17:17:24:43 [INFO    ] [ft-sam.py:312] Train: [18][10/20]	                     loss 0.13637489080429077 (0.14970865183406407	                     Acc@1 0.9921875 (0.9852430555555556
2024-11-17:17:24:45 [INFO    ] [ft-sam.py:312] Train: [18][11/20]	                     loss 0.15487182140350342 (0.150224968791008	                     Acc@1 0.9921875 (0.9859375
2024-11-17:17:24:47 [INFO    ] [ft-sam.py:312] Train: [18][12/20]	                     loss 0.11593063920736313 (0.1471073024652221	                     Acc@1 1.0 (0.9872159090909091
2024-11-17:17:24:49 [INFO    ] [ft-sam.py:312] Train: [18][13/20]	                     loss 0.1478545069694519 (0.14716956950724125	                     Acc@1 1.0 (0.98828125
2024-11-17:17:24:51 [INFO    ] [ft-sam.py:312] Train: [18][14/20]	                     loss 0.13413137197494507 (0.14616663123552615	                     Acc@1 1.0 (0.9891826923076923
2024-11-17:17:24:53 [INFO    ] [ft-sam.py:312] Train: [18][15/20]	                     loss 0.1520298719406128 (0.14658543414303235	                     Acc@1 1.0 (0.9899553571428571
2024-11-17:17:24:55 [INFO    ] [ft-sam.py:312] Train: [18][16/20]	                     loss 0.1570674628019333 (0.14728423605362576	                     Acc@1 0.984375 (0.9895833333333334
2024-11-17:17:24:57 [INFO    ] [ft-sam.py:312] Train: [18][17/20]	                     loss 0.1312149167060852 (0.14627990359440446	                     Acc@1 0.9921875 (0.98974609375
2024-11-17:17:24:59 [INFO    ] [ft-sam.py:312] Train: [18][18/20]	                     loss 0.1621638685464859 (0.14721425447393865	                     Acc@1 0.9765625 (0.9889705882352942
2024-11-17:17:25:01 [INFO    ] [ft-sam.py:312] Train: [18][19/20]	                     loss 0.1406184285879135 (0.14684781970249283	                     Acc@1 0.9921875 (0.9891493055555556
2024-11-17:17:25:03 [INFO    ] [ft-sam.py:312] Train: [18][20/20]	                     loss 0.14548079669475555 (0.14677587112313822	                     Acc@1 1.0 (0.9897203947368421
2024-11-17:17:25:04 [INFO    ] [ft-sam.py:312] Train: [18][21/20]	                     loss 0.13215668499469757 (0.14637822926044464	                     Acc@1 1.0 (0.99
2024-11-17:17:26:13 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.17021623316784978,
 'clean_test_loss_avg_over_batch': 0.2614224111732048,
 'test_acc': 0.9486,
 'test_asr': 0.9371111111111111,
 'test_ra': 0.060444444444444446,
 'train_acc': 0.99,
 'train_epoch_loss_avg_over_batch': 0.14637822926044464}
2024-11-17:17:26:16 [INFO    ] [ft-sam.py:312] Train: [19][2/20]	                     loss 0.14625827968120575 (0.14625827968120575	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:26:18 [INFO    ] [ft-sam.py:312] Train: [19][3/20]	                     loss 0.1449916660785675 (0.14562497287988663	                     Acc@1 0.984375 (0.98828125
2024-11-17:17:26:20 [INFO    ] [ft-sam.py:312] Train: [19][4/20]	                     loss 0.15383923053741455 (0.14836305876572928	                     Acc@1 0.9921875 (0.9895833333333334
2024-11-17:17:26:22 [INFO    ] [ft-sam.py:312] Train: [19][5/20]	                     loss 0.12783582508563995 (0.14323125034570694	                     Acc@1 1.0 (0.9921875
2024-11-17:17:26:24 [INFO    ] [ft-sam.py:312] Train: [19][6/20]	                     loss 0.12445420026779175 (0.1394758403301239	                     Acc@1 1.0 (0.99375
2024-11-17:17:26:26 [INFO    ] [ft-sam.py:312] Train: [19][7/20]	                     loss 0.14596953988075256 (0.140558123588562	                     Acc@1 0.9921875 (0.9934895833333334
2024-11-17:17:26:27 [INFO    ] [ft-sam.py:312] Train: [19][8/20]	                     loss 0.1312192678451538 (0.1392240013395037	                     Acc@1 0.9921875 (0.9933035714285714
2024-11-17:17:26:29 [INFO    ] [ft-sam.py:312] Train: [19][9/20]	                     loss 0.11127529293298721 (0.13573041278868914	                     Acc@1 1.0 (0.994140625
2024-11-17:17:26:31 [INFO    ] [ft-sam.py:312] Train: [19][10/20]	                     loss 0.1391710340976715 (0.13611270404524273	                     Acc@1 0.9921875 (0.9939236111111112
2024-11-17:17:26:33 [INFO    ] [ft-sam.py:312] Train: [19][11/20]	                     loss 0.1631583869457245 (0.1388172723352909	                     Acc@1 0.9921875 (0.99375
2024-11-17:17:26:35 [INFO    ] [ft-sam.py:312] Train: [19][12/20]	                     loss 0.1361033320426941 (0.13857055049050937	                     Acc@1 0.9921875 (0.9936079545454546
2024-11-17:17:26:37 [INFO    ] [ft-sam.py:312] Train: [19][13/20]	                     loss 0.13196510076522827 (0.13802009634673595	                     Acc@1 0.9921875 (0.9934895833333334
2024-11-17:17:26:39 [INFO    ] [ft-sam.py:312] Train: [19][14/20]	                     loss 0.14040689170360565 (0.13820369598957208	                     Acc@1 0.9921875 (0.9933894230769231
2024-11-17:17:26:41 [INFO    ] [ft-sam.py:312] Train: [19][15/20]	                     loss 0.15458251535892487 (0.13937361165881157	                     Acc@1 0.9921875 (0.9933035714285714
2024-11-17:17:26:43 [INFO    ] [ft-sam.py:312] Train: [19][16/20]	                     loss 0.1444910317659378 (0.13971477299928664	                     Acc@1 0.984375 (0.9927083333333333
2024-11-17:17:26:45 [INFO    ] [ft-sam.py:312] Train: [19][17/20]	                     loss 0.12995317578315735 (0.13910467317327857	                     Acc@1 0.984375 (0.9921875
2024-11-17:17:26:47 [INFO    ] [ft-sam.py:312] Train: [19][18/20]	                     loss 0.11676899343729019 (0.1377908096593969	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:26:49 [INFO    ] [ft-sam.py:312] Train: [19][19/20]	                     loss 0.14279000461101532 (0.1380685427122646	                     Acc@1 0.984375 (0.9917534722222222
2024-11-17:17:26:51 [INFO    ] [ft-sam.py:312] Train: [19][20/20]	                     loss 0.13560758531093597 (0.13793901863851046	                     Acc@1 1.0 (0.9921875
2024-11-17:17:26:52 [INFO    ] [ft-sam.py:312] Train: [19][21/20]	                     loss 0.12596438825130463 (0.13761330869197846	                     Acc@1 1.0 (0.9924
2024-11-17:17:28:02 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.1886300042691365,
 'clean_test_loss_avg_over_batch': 0.2657784798854514,
 'test_acc': 0.9499,
 'test_asr': 0.9322222222222222,
 'test_ra': 0.0651111111111111,
 'train_acc': 0.9924,
 'train_epoch_loss_avg_over_batch': 0.13761330869197846}
2024-11-17:17:28:04 [INFO    ] [ft-sam.py:312] Train: [20][2/20]	                     loss 0.14267265796661377 (0.14267265796661377	                     Acc@1 0.984375 (0.984375
2024-11-17:17:28:06 [INFO    ] [ft-sam.py:312] Train: [20][3/20]	                     loss 0.12560607492923737 (0.13413936644792557	                     Acc@1 0.984375 (0.984375
2024-11-17:17:28:08 [INFO    ] [ft-sam.py:312] Train: [20][4/20]	                     loss 0.12749846279621124 (0.13192573189735413	                     Acc@1 1.0 (0.9895833333333334
2024-11-17:17:28:10 [INFO    ] [ft-sam.py:312] Train: [20][5/20]	                     loss 0.14205841720104218 (0.13445890322327614	                     Acc@1 0.984375 (0.98828125
2024-11-17:17:28:12 [INFO    ] [ft-sam.py:312] Train: [20][6/20]	                     loss 0.13505470752716064 (0.13457806408405304	                     Acc@1 1.0 (0.990625
2024-11-17:17:28:14 [INFO    ] [ft-sam.py:312] Train: [20][7/20]	                     loss 0.11665893346071243 (0.13159154231349626	                     Acc@1 1.0 (0.9921875
2024-11-17:17:28:16 [INFO    ] [ft-sam.py:312] Train: [20][8/20]	                     loss 0.13984638452529907 (0.13277080548661097	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:28:18 [INFO    ] [ft-sam.py:312] Train: [20][9/20]	                     loss 0.12781840562820435 (0.13215175550431013	                     Acc@1 0.9921875 (0.9921875
2024-11-17:17:28:20 [INFO    ] [ft-sam.py:312] Train: [20][10/20]	                     loss 0.14348751306533813 (0.13341128412220213	                     Acc@1 0.9765625 (0.9904513888888888
2024-11-17:17:28:22 [INFO    ] [ft-sam.py:312] Train: [20][11/20]	                     loss 0.12610046565532684 (0.1326802022755146	                     Acc@1 0.9921875 (0.990625
2024-11-17:17:28:24 [INFO    ] [ft-sam.py:312] Train: [20][12/20]	                     loss 0.12008268386125565 (0.1315349733287638	                     Acc@1 1.0 (0.9914772727272727
2024-11-17:17:28:26 [INFO    ] [ft-sam.py:312] Train: [20][13/20]	                     loss 0.12647122144699097 (0.13111299400528273	                     Acc@1 0.984375 (0.9908854166666666
2024-11-17:17:28:28 [INFO    ] [ft-sam.py:312] Train: [20][14/20]	                     loss 0.13340973854064941 (0.13128966666184938	                     Acc@1 0.9921875 (0.9909855769230769
2024-11-17:17:28:30 [INFO    ] [ft-sam.py:312] Train: [20][15/20]	                     loss 0.10447005927562714 (0.12937398041997636	                     Acc@1 1.0 (0.9916294642857143
2024-11-17:17:28:32 [INFO    ] [ft-sam.py:312] Train: [20][16/20]	                     loss 0.11512620002031326 (0.12842412839333217	                     Acc@1 0.9921875 (0.9916666666666667
2024-11-17:17:28:34 [INFO    ] [ft-sam.py:312] Train: [20][17/20]	                     loss 0.13737569749355316 (0.12898360146209598	                     Acc@1 0.984375 (0.9912109375
2024-11-17:17:28:36 [INFO    ] [ft-sam.py:312] Train: [20][18/20]	                     loss 0.12504535913467407 (0.1287519401487182	                     Acc@1 1.0 (0.9917279411764706
2024-11-17:17:28:38 [INFO    ] [ft-sam.py:312] Train: [20][19/20]	                     loss 0.12317784130573273 (0.12844226799077457	                     Acc@1 1.0 (0.9921875
2024-11-17:17:28:40 [INFO    ] [ft-sam.py:312] Train: [20][20/20]	                     loss 0.13534685969352722 (0.12880566755407735	                     Acc@1 1.0 (0.9925986842105263
2024-11-17:17:28:41 [INFO    ] [ft-sam.py:312] Train: [20][21/20]	                     loss 0.11113907396793365 (0.12832513620853425	                     Acc@1 1.0 (0.9928
2024-11-17:17:29:50 [INFO    ] [trainer_cls.py:65] {'bd_test_loss_avg_over_batch': 0.18838539719581604,
 'clean_test_loss_avg_over_batch': 0.2628362443816813,
 'test_acc': 0.9502,
 'test_asr': 0.9312222222222222,
 'test_ra': 0.0661111111111111,
 'train_acc': 0.9928,
 'train_epoch_loss_avg_over_batch': 0.12832513620853425}
2024-11-17:17:29:50 [INFO    ] [save_load_attack.py:176] saving...
